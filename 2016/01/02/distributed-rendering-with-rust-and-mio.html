

<!DOCTYPE html>
<html lang="en">
<head>
	<meta charset="utf-8">
	<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="origin-trial" content="AoMftca5+Dstn4K7mCd1AKVcenGb0/EBkqJgonux6w6fVXhyJ2iHs9LuZKA36+gcaypMdBLAaIpkES6VMXoJtg8AAABQeyJvcmlnaW4iOiJodHRwczovL3d3dy53aWxsdXNoZXIuaW86NDQzIiwiZmVhdHVyZSI6IldlYkdQVSIsImV4cGlyeSI6MTY0MzE1NTE5OX0=">

	<title>Distributed Rendering with Rust and Mio</title>
	
		<meta name="description" content="">
	
    <meta name="author" content="Will Usher">

    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css" integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">

	<!-- Should I be including font-awesome from bootstrap? Or just the 5.0 version? -->
    <!--<script defer src="https://use.fontawesome.com/releases/v5.0.6/js/all.js"></script>-->
    <script defer src="https://kit.fontawesome.com/b56adde3a3.js" crossorigin="anonymous"></script>
    <script src="https://code.jquery.com/jquery-3.2.1.slim.min.js" integrity="sha384-KJ3o2DKtIkvYIK3UENzmM7KCkRr/rE9/Qpg6aAZGJwFDMVNA/GpGFF93hXpG5KkN" crossorigin="anonymous"></script>

	<link href="/assets/molokai.css" rel="stylesheet">
	<link href="/assets/custom.css" rel="stylesheet">
	<link rel="shortcut icon" href="/assets/img/identicon.ico">

</head>
<body>
	<nav class="navbar navbar-expand-md navbar-dark bg-dark static-top">
		<div class="container">
			<a class="navbar-brand" href="/">Will Usher</a>
			<button class="navbar-toggler navbar-toggler-right" type="button" data-toggle="collapse"
				data-target="#navbar_responsive" aria-controls="navbarResponsive" aria-expanded="false"
				aria-label="Toggle navigation">
					<span class="navbar-toggler-icon"></span>
			</button>
			<div class="collapse navbar-collapse" id="navbar_responsive">
				<ul class="navbar-nav mr-auto">
				
				
				  
					
					  
					  
						<li class="nav-item"><a href="/projects" class="nav-link">
							Projects</a></li>
						
					  
					
				  
					
					  
					  
						<li class="nav-item"><a href="/blog" class="nav-link">
							Blog</a></li>
						
					  
					
				  
					
					  
					  
						<li class="nav-item"><a href="/pages/sdl2" class="nav-link">
							SDL2 Tutorials</a></li>
						
					  
					
				  
					
					  
					  
						<li class="nav-item"><a href="/news" class="nav-link">
							News</a></li>
						
					  
					
				  
					
					  
					  
						<li class="nav-item"><a href="/archive" class="nav-link">
							Archive</a></li>
						
					  
					
				  
				</ul>
			</div><!-- nav.collaps -->
		</div>
	</nav>

	<div class="container content mb-4">
		

<h1 class="display-4">Distributed Rendering with Rust and Mio</h1>

<div class="row justify-content-center">
	<div class="col-9">
        <p>January  2, 2016</p>
		
<p>In this post we’ll take a look at adding distributed rendering to
<a href="https://github.com/Twinklebear/tray_rust">tray_rust</a> which will let us take advantage of multiple
machines when rendering an image, like a compute cluster.
To do this we’ll look at options for how to distribute the rendering job across multiple nodes
and what sort of communication is needed synchronize their work. We’ll also look into how we
can use <a href="https://github.com/carllerche/mio">mio</a> to write an efficient master process that
can manage multiple workers effectively.</p>

<p>After implementing a simple technique to distribute the job we’ll discuss
the scalability of this approach and possible paths forward to improve it. I’ve
also recently written a <a href="https://github.com/Twinklebear/tray_rust_blender">plugin for Blender</a> so you can
easily create your own scenes and
will mention a bit on how to run the ray tracer on Google Compute Engine (or AWS EC2)
if you want to try out the distributed rendering yourself.</p>

<!--more-->

<h1 id="distributing-work-in-a-ray-tracer">Distributing Work in a Ray Tracer</h1>

<p>To motivate how to distribute the rendering job between nodes it’s worth a short review
of how work is distributed between threads on a single machine.</p>

<h2 id="work-distribution-between-threads">Work Distribution Between Threads</h2>

<p>Since all the work
of tracing a ray through a pixel and into the scene is independent for each pixel a simple approach
is to split the image up into blocks, let’s say 8x8, and assign these blocks
to different threads to render. We end up with a simple queue of blocks to be rendered and track the
next one to hand out with an atomic counter. When a thread wants a new block to render it just
increments the counter and fetches the next block, or sees that there are none left and exits. Choosing a
block size bigger than 1x1 (a single pixel) but not too big gives a nice trade off between load balancing
blocks and synchronization overhead when a thread gets its next block.
In an attempt to get some cache-coherence the block queue is sorted in
<a href="https://en.wikipedia.org/wiki/Z-order_curve">Z-order</a> so it’s more likely a thread will pick a
block closer to what it and other threads are working on, giving a higher chance the meshes
and other data for that region are already in cache.</p>

<div class="col-md-12">
<div class="col-md-8 offset-md-2 text-md-center">
<pre class="diagram">
                +----------+----------+
                |          |          |
                |          |          |
   Given to ---&gt;+ block 0  | block 1  |&lt;--- Given to
   thread 0     |        --+-.        |     thread 1
                |          |/         |
                +----------/----------+
                |         /|          |
                |        '-+--        |
 Next block ---&gt;| block 2  | block 3  |
 to give out    |          |          |
                |          |          |
                +----------+----------+
</pre>
<i>Distributing blocks in Z-order for a 16x16 image rendered with two threads.
Diagrams generated with <a href="http://casual-effects.com/markdeep/">Markeep</a>.</i>
</div>
</div>

<p>Saving the sampled color data from a ray in this system is also relatively simple, but requires a bit of
synchronization to handle <a href="http://www.luxrender.net/wiki/LuxRender_Render_settings#Filter">reconstruction filtering</a>.
When using a reconstruction filter the sample
computed by a ray is written to multiple pixels, weighted by the filter weight at each pixel.
Threads must synchronize their writes since the blocks of pixels they’re writing to are not disjoint even
though the blocks they’re assigned are.
This synchronization is handled with a fine-grained locking scheme, where each 2x2 block of pixels
in the framebuffer is protected by a mutex that the thread must acquire before writing its samples to that
region of the framebuffer. Without reconstruction filtering threads never write to the same pixel since
their work regions are disjoint and we wouldn’t need any synchronization, however the quality improvement
from reconstruction filtering is worth the added overhead.</p>

<blockquote>
  <p><em>Note:</em> A pixel in the framebuffer isn’t an 8-bit RGB triple since we need
the filter weight of each pixel accumulated from samples written to it.
A pixel is four floats, an RGB triple and the accumulated filter weight.
To save the image we divide the RGB components by the filter weight and convert to
8-bit sRGB. Without storing the floating point samples we wouldn’t be able
to correctly reconstruct the image when taking multiple samples per pixel and/or using reconstruction
filtering.</p>
</blockquote>

<h2 id="extending-to-multiple-machines">Extending to Multiple Machines</h2>

<p>This image based work decomposition extends easily to distributing work across multiple
machines. Instead of having a single node work through the block queue we assign
subsets of it to different machines in our cluster, where they will distribute
their blocks to threads for rendering as before. By adding the reasonable
assumption that the scene data is on some shared filesystem or has already been uploaded to each
machine we can send a simple set of instructions to each worker. All we need to tell them
is where to find the scene file, which frames of the animation to render and what subset of blocks
they’ve been assigned.</p>

<p>Suppose we’re rendering a 400x304 image of the scene “cornell_box.json” with five
worker nodes. In total we have 1900 8x8 blocks to render and the master
will instruct each node to render 380 of them. In the case that
the number of nodes doesn’t evenly divide the number of blocks the remainder are given to the
last worker.</p>

<div class="col-md-12 text-md-center">
<pre class="diagram">
       .-------------------------.      .---------.       .-------------------------.       
      | Scene: "cornell_box.json" |     |         |      | Scene: "cornell_box.json" |      
      | Frames: (0, 0)            |     | Master  |      | Frames: (0, 0)            |      
      | Block Start: 0            |     +---------+      | Block Start: 1520         |      
      | Block Count: 380          |    /// ----- \\\     | Block Count: 380          |      
       '-----------+-------------'    '--+-+-+-+-+--'     '--------+----------------'       
                   |                     | | | | |                 |                        
              .----o--------------------'  | | |  '----------------o--------.               
             |                .-----------'  |  '-----------.                |              
             |               |               |               |               |              
             v               v               v               v               v              
         .---+----.      .---+----.      .---+----.      .---+----.      .---+----.         
        /        /|     /        /|     /        /|     /        /|     /        /|         
       +--------+/|    +--------+/|    +--------+/|    +--------+/|    +--------+/|         
       |        +/|    |        +/|    |        +/|    |        +/|    |        +/|         
       | Worker +/|    | Worker +/|    | Worker +/|    | Worker +/|    | Worker +/|         
       | node 0 +/|    | node 1 +/|    | node 2 +/|    | node 3 +/|    | node 4 +/|         
       |        +/     |        +/     |        +/     |        +/     |        +/          
       '--------'      '--------'      '--------'      '--------'      '--------'           
</pre>
<i>Master sends instructions to the worker nodes</i>
</div>

<h3 id="combining-results-from-workers">Combining Results from Workers</h3>

<p>We’re left with one small issue when combining results from our worker nodes, and it relates
to why we need store floating point sample data and deal with locking our framebuffer to synchronize it
between threads on a node. Although the regions assigned to workers are disjoint subsets of the
image the samples that their threads compute will overlap with neighboring nodes due to reconstruction
filtering. For example, if we use a 4x4 pixel filter each node
will write to its 8x8 blocks and +/- 2 pixels in each direction, overlapping with other worker’s regions.</p>

<p>In our example rendering of the Cornell box the images below would be the results of each worker if
we just saved them out. If you zoom in a bit you can see there’s some overlap between the regions written to by
each worker. The images are a bit noisy since they were rendered with just 256 samples per pixel.
Note that with the Mitchell-Netravali
filter used here some pixels are actually assigned negative weights so these won’t show up properly in the
images below. Additionally, there may also be a bug causing the odd extra boxes at the corners of the worker’s
results, I need to look into this some more.</p>

<div class="row">
<div class="col-md-6 text-md-center">
<img class="img-fluid" src="https://cdn.willusher.io/img/9UfVGse.webp" alt="Node 0's results" />
<i>Worker 0's Results</i>
</div>
<div class="col-md-6 text-md-center">
<img class="img-fluid" src="https://cdn.willusher.io/img/1PrdC7N.webp" alt="node 1's results" />
<i>Worker 1's Results</i>
</div>
</div>

<div class="row">
<div class="col-md-4 text-md-center">
<img class="img-fluid" src="https://cdn.willusher.io/img/NKXW7ap.webp" alt="node 2's results" />
<i>Worker 2's Results</i>
</div>

<div class="col-md-4 text-md-center">
<img class="img-fluid" src="https://cdn.willusher.io/img/lF8jwRE.webp" alt="node 3's results" />
<i>Worker 3's Results</i>
</div>

<div class="col-md-4 text-md-center">
<img class="img-fluid" src="https://cdn.willusher.io/img/Qz7yIyL.webp" alt="node 4's results" />
<i>Worker 4's Results</i>
</div>
</div>

<p>Due to this overlap the master node needs to be able to properly combine pixels from multiple nodes.
This requires that each worker sends its pixels as the full float RGB triple and filter weight that
we mentioned earlier.</p>

<h3 id="reducing-communication-overhead">Reducing Communication Overhead</h3>

<p>Since most of the worker’s framebuffer will be black as other nodes are assigned those regions there’s no
reason to send the whole thing. This significantly reduces communication overhead, sending the full
framebuffer would significantly increase the data transferred as we added more nodes which will hurt scalability.
Consider rendering a 1920x1080 image: each node sends the master a RGBW float framebuffer
for a total of about 32MB per node. If we used 64 nodes to render this we’d send a total of about 2.05GB,
where only about 1.56% of each 32MB framebuffer sent by the workers has actually been written too!</p>

<p>Since the worker only writes to a small portion of the framebuffer we can just send the blocks that it’s
actually written too.
For convenience I currently send blocks of the same size as the framebuffer locks, so each worker will send
a list of 2x2 blocks along with a list of where the blocks are in the image. This significantly reduces
communication overhead and provides the master with all the information it needs to place a worker’s results
in the final framebuffer. This does add some overhead to each block: we send the x,y coordinates of the
block as unsigned 64-bit ints which adds a 16 byte header to each 2x2 block (64 bytes of pixel data) for
a 25% overhead per block.</p>

<p>We do still add some additional communication as we add more nodes since the regions that nodes
send back to the master overlap some based on the size of the reconstruction filter. We’ll discuss
one idea for potentially reducing this overhead further in the scalability section.</p>

<div class="col-md-12 text-md-center">
<pre class="diagram">
        .--------.      .--------.      .--------.      .--------.      .--------.      
       /        /|     /        /|     /        /|     /        /|     /        /|      
      +--------+/|    +--------+/|    +--------+/|    +--------+/|    +--------+/|      
      |        +/|    |        +/|    |        +/|    |        +/|    |        +/|      
      | Worker +/|    | Worker +/|    | Worker +/|    | Worker +/|    | Worker +/|      
      | node 0 +/|    | node 1 +/|    | node 2 +/|    | node 3 +/|    | node 4 +/|      
      |        +/     |        +/     |        +/     |        +/     |        +/       
      '----+---'      '----+---'      '----+---'      '----+---'      '----+---'        
           |               |               |               |               |            
           |               |               |               |               |            
           |               |               |               |               |            
           |                '----------.   |   .----------'                |            
            '---------o--------------.  |  |  |  .------------------------'             
                      |               | |  |  | |                                       
   .------------------+---------.     v v  v  v v                                       
  | Frame: 0                     |    .---------.                                       
  | Block Size: (2, 2)           |    |         |                                       
  | Blocks: [(0, 0), (1, 0),...] |    | Master  |                                       
  | Pixels: [8.2, 7.6, 2, 9,...] |    +---------+                                       
   '----------------------------'    /// ----- \\\                                      
                                    '------+------'                                     
                                           |                                            
                                           v                                            
                                     +-----------+                                      
                                     | Rendered  |                                      
                                     |  Image    |                                      
                                     +-----------+                                      
</pre>
<i>Master collects rendering results from the workers and saves the image</i>
</div>

<p>Once the master has collected results from all the workers it can do the final filter weight
division for each pixel and conversion to sRGB to save the frame out as a PNG. Here’s the result
for our 400x304 Cornell box example with 256 samples per pixel that we’ve been following along.</p>

<div class="col-md-12 text-md-center">
<img class="img-fluid" src="https://cdn.willusher.io/img/YEhp254.webp" alt="Rendered result" />
<i>Rendered Image</i>
</div>

<h1 id="coordinating-the-workers">Coordinating the Workers</h1>

<p>The master node requires some more work to implement than the workers, it needs to manage
accepting data from multiple workers who are reporting different regions of (potentially different) frames
of the animation and combining these results. It also needs to track whether a frame has been completed and
can be saved out to disk. Additionally we’d like the master to be as lightweight as possible so it can
run on the same node as one of the workers and not take up too much CPU.</p>

<p>We’d also like to avoid requiring workers to open a new TCP connection each time they want to send
results to the master node. Having the master node listen on a TCP socket and behave as a blocking server
that accepts connections from workers and reads data from one at a time likely won’t scale up very well.
What we’d really like is some kind of async event loop,
where the master can wait on data from multiple workers at the same time and read from them when they’ve
sent some data.</p>

<h2 id="asynchronous-io-with-mio">Asynchronous I/O with mio</h2>

<p>This is where <a href="https://github.com/carllerche/mio">mio</a> comes in. Mio is a powerful low overhead I/O library
for Rust and most important for us, it provides abstractions for non-blocking TCP sockets and an event loop for
reading/writing based on whether they’re readable/writable or not. Having never used non-blocking sockets
or event based I/O this took a bit of learning but has turned out very nice.</p>

<p>When constructing the master we can start a TCP connection to each worker,
who are all listening on the same port. The master is given the list of worker hostnames or IP addresses
specifying the worker nodes to contact. To identify the connection that an event was received on
mio allows you to specify a <a href="http://rustdoc.s3-website-us-east-1.amazonaws.com/mio/master/mio/struct.Token.html">Token</a>
with a <code class="highlighter-rouge">usize</code>, we’ll just use the worker’s index in the list of workers.</p>

<figure class="highlight"><pre><code class="language-rust" data-lang="rust"><span class="c">// Loop through the list of workers and connect to them.
</span>
<span class="k">for</span> <span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">host</span><span class="p">)</span> <span class="n">in</span> <span class="n">workers</span><span class="nf">.iter</span><span class="p">()</span><span class="nf">.enumerate</span><span class="p">()</span> <span class="p">{</span>
    <span class="c">// to_socket_addrs returns a Result&lt;Iterator&gt; of SocketAddrs, if we fail parsing
</span>
    <span class="c">// we should just abort so we use unwrap here.
</span>
    <span class="k">let</span> <span class="n">addr</span> <span class="o">=</span> <span class="p">(</span><span class="o">&amp;</span><span class="n">host</span><span class="p">[</span><span class="o">..</span><span class="p">],</span> <span class="nn">worker</span><span class="p">::</span><span class="n">PORT</span><span class="p">)</span><span class="nf">.to_socket_addrs</span><span class="p">()</span><span class="nf">.unwrap</span><span class="p">()</span><span class="nf">.next</span><span class="p">()</span><span class="nf">.unwrap</span><span class="p">();</span>
    <span class="c">// Use mio's TcpStream to connect to the worker, if we succeed in starting the
</span>
    <span class="c">// connection add this stream to the event loop
</span>
    <span class="k">match</span> <span class="nn">TcpStream</span><span class="p">::</span><span class="nf">connect</span><span class="p">(</span><span class="o">&amp;</span><span class="n">addr</span><span class="p">)</span> <span class="p">{</span>
        <span class="nf">Ok</span><span class="p">(</span><span class="n">stream</span><span class="p">)</span> <span class="k">=&gt;</span> <span class="p">{</span>
            <span class="c">// Each worker is identified in the event loop by their index in the workers Vec
</span>
            <span class="k">match</span> <span class="n">event_loop</span><span class="nf">.register</span><span class="p">(</span><span class="o">&amp;</span><span class="n">stream</span><span class="p">,</span> <span class="nf">Token</span><span class="p">(</span><span class="n">i</span><span class="p">),</span> <span class="nn">EventSet</span><span class="p">::</span><span class="nf">all</span><span class="p">(),</span> <span class="nn">PollOpt</span><span class="p">::</span><span class="nf">level</span><span class="p">()){</span>
                <span class="nf">Err</span><span class="p">(</span><span class="n">e</span><span class="p">)</span> <span class="k">=&gt;</span> <span class="nd">panic!</span><span class="p">(</span><span class="s">"Error registering stream from {}: {}"</span><span class="p">,</span> <span class="n">host</span><span class="p">,</span> <span class="n">e</span><span class="p">),</span>
                <span class="nf">Ok</span><span class="p">(</span><span class="mi">_</span><span class="p">)</span> <span class="k">=&gt;</span> <span class="p">{},</span>
            <span class="p">}</span>
            <span class="n">connections</span><span class="nf">.push</span><span class="p">(</span><span class="n">stream</span><span class="p">);</span>
        <span class="p">},</span>
        <span class="nf">Err</span><span class="p">(</span><span class="n">e</span><span class="p">)</span> <span class="k">=&gt;</span> <span class="nd">panic!</span><span class="p">(</span><span class="s">"Failed to contact worker {}: {:?}"</span><span class="p">,</span> <span class="n">host</span><span class="p">,</span> <span class="n">e</span><span class="p">),</span>
    <span class="p">}</span>
<span class="p">}</span></code></pre></figure>

<p>After constructing the master we start running mio’s event loop. Our Master struct implements the
<a href="http://rustdoc.s3-website-us-east-1.amazonaws.com/mio/master/mio/trait.Handler.html"><code class="highlighter-rouge">mio::Handler</code></a>
trait which requires we implement the <code class="highlighter-rouge">ready</code> method. This method is called by mio when an event
is ready to be handled on one of the TCP streams we registered with the event loop when constructing
the master.</p>

<figure class="highlight"><pre><code class="language-rust" data-lang="rust"><span class="k">impl</span> <span class="n">Handler</span> <span class="k">for</span> <span class="n">Master</span> <span class="p">{</span>
    <span class="k">type</span> <span class="n">Timeout</span> <span class="o">=</span> <span class="p">();</span>
    <span class="k">type</span> <span class="n">Message</span> <span class="o">=</span> <span class="p">();</span>

    <span class="k">fn</span> <span class="nf">ready</span><span class="p">(</span><span class="o">&amp;</span><span class="k">mut</span> <span class="k">self</span><span class="p">,</span> <span class="n">event_loop</span><span class="p">:</span> <span class="o">&amp;</span><span class="k">mut</span> <span class="n">EventLoop</span><span class="o">&lt;</span><span class="n">Master</span><span class="o">&gt;</span><span class="p">,</span> <span class="n">token</span><span class="p">:</span> <span class="n">Token</span><span class="p">,</span> <span class="n">event</span><span class="p">:</span> <span class="n">EventSet</span><span class="p">)</span> <span class="p">{</span>
        <span class="k">let</span> <span class="n">worker</span> <span class="o">=</span> <span class="n">token</span><span class="nf">.as_usize</span><span class="p">();</span>
        <span class="k">if</span> <span class="n">event</span><span class="nf">.is_writable</span><span class="p">()</span> <span class="p">{</span>
            <span class="c">// Send the worker their instructions and stop listening for writable events
</span>
        <span class="p">}</span>
        <span class="k">if</span> <span class="n">event</span><span class="nf">.is_readable</span><span class="p">()</span> <span class="p">{</span>
            <span class="c">// Read frame data sent by the worker
</span>
        <span class="p">}</span>
    <span class="p">}</span>
<span class="p">}</span></code></pre></figure>

<p>The writable event handling isn’t too interesting as we just send the worker their instructions
using blocking I/O. The instructions struct with the worker’s instructions is encoded
using <a href="https://github.com/TyOverby/bincode">bincode</a> and sent
with <a href="http://doc.rust-lang.org/std/io/trait.Write.html#method.write_all">write_all</a>. The readable
event handling is where the more interesting work of dealing with async writes from multiple workers is done.</p>

<h2 id="reading-from-multiple-workers-asynchronously">Reading From Multiple Workers Asynchronously</h2>

<p>While we’ve reduced the communication overhead quite a bit the workers are still going to be sending
more data than we would get with a single call to
<a href="http://doc.rust-lang.org/std/io/trait.Read.html#tymethod.read">read</a>, if we wanted to read the
entire frame data sent by the worker immediately upon receiving a readable event we’d still need to
block. However blocking ruins the whole reason we’ve gone with mio and async I/O in the first place!
While we’re stuck blocking on this worker there’s incoming writes from other workers that we should
also be reading from.</p>

<p>The solution I’ve chosen here is to have a buffer for each worker that we write to each
time we get a new readable event, tracking how many bytes we’ve read so far and how many we expect to read
in total.
Each time we get a readable event on the worker we do a single <code class="highlighter-rouge">read</code> to get the bytes currently available
without blocking and place these in the buffer starting after the data we’ve read
previously. Once we’ve read all the bytes the worker is sending us (our data starts with an unsigned
64 bit int specifying the number sent) we can decode the frame with bincode.</p>

<h3 id="the-worker-buffer">The Worker Buffer</h3>

<p>A worker buffer is a partially read result from a worker, where we’re still waiting on more bytes
from the network or TCP stack. It tracks how many bytes we’re expecting to receive and how many we’ve
gotten so far so we know where to continue writing to the buffer and when we’ve got everything.</p>

<figure class="highlight"><pre><code class="language-rust" data-lang="rust"><span class="k">struct</span> <span class="n">WorkerBuffer</span> <span class="p">{</span>
    <span class="k">pub</span> <span class="n">buf</span><span class="p">:</span> <span class="nb">Vec</span><span class="o">&lt;</span><span class="nb">u8</span><span class="o">&gt;</span><span class="p">,</span>
    <span class="k">pub</span> <span class="n">expected_size</span><span class="p">:</span> <span class="nb">usize</span><span class="p">,</span>
    <span class="k">pub</span> <span class="n">currently_read</span><span class="p">:</span> <span class="nb">usize</span><span class="p">,</span>
<span class="p">}</span>
<span class="k">impl</span> <span class="n">WorkerBuffer</span> <span class="p">{</span>
    <span class="k">pub</span> <span class="k">fn</span> <span class="nf">new</span><span class="p">()</span> <span class="k">-&gt;</span> <span class="n">WorkerBuffer</span> <span class="p">{</span>
        <span class="c">// We start with an expected size of 8 since we expect a 64 bit uint header
</span>
        <span class="c">// telling us how many bytes are being sent
</span>
        <span class="n">WorkerBuffer</span> <span class="p">{</span> <span class="n">buf</span><span class="p">:</span> <span class="nn">Vec</span><span class="p">::</span><span class="nf">new</span><span class="p">(),</span> <span class="n">expected_size</span><span class="p">:</span> <span class="mi">8</span><span class="p">,</span> <span class="n">currently_read</span><span class="p">:</span> <span class="mi">0</span> <span class="p">}</span>
    <span class="p">}</span>
<span class="p">}</span></code></pre></figure>

<p>The buffer stores a partially read <code class="highlighter-rouge">Frame</code> sent back by a worker to report its results. The <code class="highlighter-rouge">Frame</code> struct
begins with an 8 byte header specifying how many bytes it is when encoded with bincode.</p>

<figure class="highlight"><pre><code class="language-rust" data-lang="rust"><span class="k">struct</span> <span class="n">Frame</span> <span class="p">{</span>
    <span class="k">pub</span> <span class="n">encoded_size</span><span class="p">:</span> <span class="nb">u64</span><span class="p">,</span>
    <span class="c">// Which frame the worker is sending its results for
</span>
    <span class="k">pub</span> <span class="n">frame</span><span class="p">:</span> <span class="nb">usize</span><span class="p">,</span>
    <span class="c">// Block size of the blocks being sent
</span>
    <span class="k">pub</span> <span class="n">block_size</span><span class="p">:</span> <span class="p">(</span><span class="nb">usize</span><span class="p">,</span> <span class="nb">usize</span><span class="p">),</span>
    <span class="c">// Starting locations of each block
</span>
    <span class="k">pub</span> <span class="n">blocks</span><span class="p">:</span> <span class="nb">Vec</span><span class="o">&lt;</span><span class="p">(</span><span class="nb">usize</span><span class="p">,</span> <span class="nb">usize</span><span class="p">)</span><span class="o">&gt;</span><span class="p">,</span>
    <span class="c">// Sample data for each block, RGBW
</span>
    <span class="k">pub</span> <span class="n">pixels</span><span class="p">:</span> <span class="nb">Vec</span><span class="o">&lt;</span><span class="nb">f32</span><span class="o">&gt;</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>

<h3 id="reading-into-a-worker-buffer">Reading into a Worker Buffer</h3>

<p>When the master gets a readable event on a worker it adds the newly available bytes to the worker’s buffer
and checks if it’s read the entire <code class="highlighter-rouge">Frame</code>. This is handled by the <code class="highlighter-rouge">read_worker_buffer</code> method which returns
true if we’ve got all the data the worker is sending and can decode the frame. This code is best
explained inline with comments, so please see the snippet for details on how this works.</p>

<figure class="highlight"><pre><code class="language-rust" data-lang="rust"><span class="k">fn</span> <span class="nf">read_worker_buffer</span><span class="p">(</span><span class="o">&amp;</span><span class="k">mut</span> <span class="k">self</span><span class="p">,</span> <span class="n">worker</span><span class="p">:</span> <span class="nb">usize</span><span class="p">)</span> <span class="k">-&gt;</span> <span class="nb">bool</span> <span class="p">{</span>
    <span class="k">let</span> <span class="n">buf</span> <span class="o">=</span> <span class="o">&amp;</span><span class="k">mut</span> <span class="k">self</span><span class="py">.worker_buffers</span><span class="p">[</span><span class="n">worker</span><span class="p">];</span>
    <span class="c">// If we haven't read the size of data being sent, read that now
</span>
    <span class="k">if</span> <span class="n">buf</span><span class="py">.currently_read</span> <span class="o">&lt;</span> <span class="mi">8</span> <span class="p">{</span>
        <span class="c">// First 8 bytes are a u64 specifying the number of bytes being sent,
</span>
        <span class="c">// make sure we have room to store 8 bytes
</span>
        <span class="n">buf</span><span class="py">.buf</span><span class="nf">.extend</span><span class="p">(</span><span class="nn">iter</span><span class="p">::</span><span class="nf">repeat</span><span class="p">(</span><span class="mi">0u8</span><span class="p">)</span><span class="nf">.take</span><span class="p">(</span><span class="mi">8</span><span class="p">));</span>
        <span class="k">match</span> <span class="k">self</span><span class="py">.connections</span><span class="p">[</span><span class="n">worker</span><span class="p">]</span><span class="nf">.read</span><span class="p">(</span><span class="o">&amp;</span><span class="k">mut</span> <span class="n">buf</span><span class="py">.buf</span><span class="p">[</span><span class="n">buf</span><span class="py">.currently_read</span><span class="o">..</span><span class="p">])</span> <span class="p">{</span>
            <span class="nf">Ok</span><span class="p">(</span><span class="n">n</span><span class="p">)</span> <span class="k">=&gt;</span> <span class="n">buf</span><span class="py">.currently_read</span> <span class="o">+=</span> <span class="n">n</span><span class="p">,</span>
            <span class="nf">Err</span><span class="p">(</span><span class="n">e</span><span class="p">)</span> <span class="k">=&gt;</span> <span class="nd">println!</span><span class="p">(</span><span class="s">"Error reading results from worker {}: {}"</span><span class="p">,</span> <span class="k">self</span><span class="py">.workers</span><span class="p">[</span><span class="n">worker</span><span class="p">],</span> <span class="n">e</span><span class="p">),</span>
        <span class="p">}</span>
        <span class="c">// Recall that our initial expected size is 8, for the size header.
</span>
        <span class="c">// Once we've read this we can decode the header and get the real size of the data.
</span>
        <span class="k">if</span> <span class="n">buf</span><span class="py">.currently_read</span> <span class="o">==</span> <span class="n">buf</span><span class="py">.expected_size</span> <span class="p">{</span>
            <span class="n">buf</span><span class="py">.expected_size</span> <span class="o">=</span> <span class="nf">decode</span><span class="p">(</span><span class="o">&amp;</span><span class="n">buf</span><span class="py">.buf</span><span class="p">[</span><span class="o">..</span><span class="p">])</span><span class="nf">.unwrap</span><span class="p">();</span>
            <span class="c">// Extend the Vec so we've got room for the remaning bytes, minus 8 for the
</span>
            <span class="c">// size header we just read
</span>
            <span class="n">buf</span><span class="py">.buf</span><span class="nf">.extend</span><span class="p">(</span><span class="nn">iter</span><span class="p">::</span><span class="nf">repeat</span><span class="p">(</span><span class="mi">0u8</span><span class="p">)</span><span class="nf">.take</span><span class="p">(</span><span class="n">buf</span><span class="py">.expected_size</span> <span class="o">-</span> <span class="mi">8</span><span class="p">));</span>
        <span class="p">}</span>
    <span class="p">}</span>
    <span class="c">// If we've finished reading the size header we can now start reading the frame data
</span>
    <span class="k">if</span> <span class="n">buf</span><span class="py">.currently_read</span> <span class="o">&gt;=</span> <span class="mi">8</span> <span class="p">{</span>
        <span class="k">match</span> <span class="k">self</span><span class="py">.connections</span><span class="p">[</span><span class="n">worker</span><span class="p">]</span><span class="nf">.read</span><span class="p">(</span><span class="o">&amp;</span><span class="k">mut</span> <span class="n">buf</span><span class="py">.buf</span><span class="p">[</span><span class="n">buf</span><span class="py">.currently_read</span><span class="o">..</span><span class="p">])</span> <span class="p">{</span>
            <span class="nf">Ok</span><span class="p">(</span><span class="n">n</span><span class="p">)</span> <span class="k">=&gt;</span> <span class="n">buf</span><span class="py">.currently_read</span> <span class="o">+=</span> <span class="n">n</span><span class="p">,</span>
            <span class="nf">Err</span><span class="p">(</span><span class="n">e</span><span class="p">)</span> <span class="k">=&gt;</span> <span class="nd">println!</span><span class="p">(</span><span class="s">"Error reading results from worker {}: {}"</span><span class="p">,</span> <span class="k">self</span><span class="py">.workers</span><span class="p">[</span><span class="n">worker</span><span class="p">],</span> <span class="n">e</span><span class="p">),</span>
        <span class="p">}</span>
    <span class="p">}</span>
    <span class="c">// Return true if we've read all the data we're expecting
</span>
    <span class="n">buf</span><span class="py">.currently_read</span> <span class="o">==</span> <span class="n">buf</span><span class="py">.expected_size</span>
<span class="p">}</span></code></pre></figure>

<p>In the master’s event loop we call this function on readable events, passing the ID of the worker we’re
reading from.
If we’ve read the all the data being sent by the worker we decode the frame
with bincode, accumulate its data into the combined frame and clean up to start
reading the next frame.</p>

<figure class="highlight"><pre><code class="language-rust" data-lang="rust"><span class="c">// Read results from the worker, if we've accumulated all the data being sent
</span>
<span class="c">// decode and accumulate the frame
</span>
<span class="k">if</span> <span class="k">self</span><span class="nf">.read_worker_buffer</span><span class="p">(</span><span class="n">worker</span><span class="p">)</span> <span class="p">{</span>
    <span class="k">let</span> <span class="n">frame</span> <span class="o">=</span> <span class="nf">decode</span><span class="p">(</span><span class="o">&amp;</span><span class="k">self</span><span class="py">.worker_buffers</span><span class="p">[</span><span class="n">worker</span><span class="p">]</span><span class="py">.buf</span><span class="p">[</span><span class="o">..</span><span class="p">])</span><span class="nf">.unwrap</span><span class="p">();</span>
    <span class="k">self</span><span class="nf">.save_results</span><span class="p">(</span><span class="n">frame</span><span class="p">);</span>
    <span class="c">// Clear the worker buffer for the next frame
</span>
    <span class="k">self</span><span class="py">.worker_buffers</span><span class="p">[</span><span class="n">worker</span><span class="p">]</span><span class="py">.buf</span><span class="nf">.clear</span><span class="p">();</span>
    <span class="k">self</span><span class="py">.worker_buffers</span><span class="p">[</span><span class="n">worker</span><span class="p">]</span><span class="py">.expected_size</span> <span class="o">=</span> <span class="mi">8</span><span class="p">;</span>
    <span class="k">self</span><span class="py">.worker_buffers</span><span class="p">[</span><span class="n">worker</span><span class="p">]</span><span class="py">.currently_read</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span></code></pre></figure>

<h3 id="handling-workers-reporting-different-frames">Handling Workers Reporting Different Frames</h3>

<p>Due to differences in scene complexity or the worker node hardware some workers may
finish their assigned blocks faster or slower than others. The master node can’t assume that once it starts
getting frame one from a worker that all workers have reported frame zero and it can be saved out. We need
to track how many workers have reported a frame as we accumulate results from them.
Only when all workers have reported their results for a frame can we save it out and mark it completed.</p>

<p>This is expressible very nicely with the enum type in Rust. A frame can either be in progress and we’re
tracking how many workers have reported it and the image where we’re accumulating results
or it’s completed and has been saved out.</p>

<figure class="highlight"><pre><code class="language-rust" data-lang="rust"><span class="k">enum</span> <span class="n">DistributedFrame</span> <span class="p">{</span>
    <span class="n">InProgress</span> <span class="p">{</span>
        <span class="c">// The number of workers who have reported results for this frame so far
</span>
        <span class="n">num_reporting</span><span class="p">:</span> <span class="nb">usize</span><span class="p">,</span>
        <span class="c">// Where we're combining worker's results to form the final image
</span>
        <span class="n">render</span><span class="p">:</span> <span class="n">Image</span><span class="p">,</span>
    <span class="p">},</span>
    <span class="n">Completed</span><span class="p">,</span>
<span class="p">}</span>

<span class="k">impl</span> <span class="n">DistributedFrame</span> <span class="p">{</span>
    <span class="c">// Start accumulating results for a frame, we begin with no workers reporting
</span>
    <span class="k">pub</span> <span class="k">fn</span> <span class="nf">start</span><span class="p">(</span><span class="n">img_dim</span><span class="p">:</span> <span class="p">(</span><span class="nb">usize</span><span class="p">,</span> <span class="nb">usize</span><span class="p">))</span> <span class="k">-&gt;</span> <span class="n">DistributedFrame</span> <span class="p">{</span>
        <span class="nn">DistributedFrame</span><span class="p">::</span><span class="n">InProgress</span> <span class="p">{</span> <span class="n">num_reporting</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="n">render</span><span class="p">:</span> <span class="nn">Image</span><span class="p">::</span><span class="nf">new</span><span class="p">(</span><span class="n">img_dim</span><span class="p">)</span> <span class="p">}</span>
    <span class="p">}</span>
<span class="p">}</span></code></pre></figure>

<p>The master stores a <code class="highlighter-rouge">HashMap&lt;usize, DistributedFrame&gt;</code> which maps frame numbers to distributed frames
so we can quickly look up a frame when we’ve
decoded one from a worker to accumulate its results into the final image. When we try to look up
a frame in the map there’s two possibilities: the frame could be in the map (and is either <code class="highlighter-rouge">InProgress</code>
or <code class="highlighter-rouge">Completed</code>) or this worker is the first to report this frame and we must create the entry.</p>

<p>This operation is performed with the hash map
<a href="http://doc.rust-lang.org/std/collections/hash_map/enum.Entry.html"><code class="highlighter-rouge">Entry</code></a> API. It’s important to note
that we use the <code class="highlighter-rouge">or_insert_with</code> method instead of just <code class="highlighter-rouge">or_insert</code> because starting a distributed frame
involves allocating a new image to store the result. By passing a function to call instead of a value to
insert we don’t need to start a new distributed frame each time we look one up, just when we find that
it’s not in the map.</p>

<figure class="highlight"><pre><code class="language-rust" data-lang="rust"><span class="k">let</span> <span class="k">mut</span> <span class="n">df</span> <span class="o">=</span> <span class="k">self</span><span class="py">.frames</span><span class="nf">.entry</span><span class="p">(</span><span class="n">frame_num</span><span class="p">)</span><span class="nf">.or_insert_with</span><span class="p">(||</span> <span class="nn">DistributedFrame</span><span class="p">::</span><span class="nf">start</span><span class="p">(</span><span class="n">img_dim</span><span class="p">));</span></code></pre></figure>

<p>Next we handle the two cases of the entry existing in the map, either it’s in progress and we can
accumulate the results from the worker or it’s been marked completed and something has gone wrong.
If we’ve finished an in progress frame we save it to disk and mark that we’ve finished it by setting
it to <code class="highlighter-rouge">Completed</code> outside the match. A frame is determined to be completed if the number of workers
who’ve reported results for it is equal to the total number of workers.</p>

<figure class="highlight"><pre><code class="language-rust" data-lang="rust"><span class="k">let</span> <span class="k">mut</span> <span class="n">finished</span> <span class="o">=</span> <span class="k">false</span><span class="p">;</span>
<span class="k">match</span> <span class="n">df</span> <span class="p">{</span>
    <span class="o">&amp;</span><span class="k">mut</span> <span class="nn">DistributedFrame</span><span class="p">::</span><span class="n">InProgress</span> <span class="p">{</span> <span class="k">ref</span> <span class="k">mut</span> <span class="n">num_reporting</span><span class="p">,</span> <span class="k">ref</span> <span class="k">mut</span> <span class="n">render</span> <span class="p">}</span> <span class="k">=&gt;</span> <span class="p">{</span>
        <span class="c">// Collect results from the worker and see if we've finished the
</span>
        <span class="c">// frame and can save it
</span>
        <span class="n">render</span><span class="nf">.add_blocks</span><span class="p">(</span><span class="n">frame</span><span class="py">.block_size</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">frame</span><span class="py">.blocks</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">frame</span><span class="py">.pixels</span><span class="p">);</span>
        <span class="o">*</span><span class="n">num_reporting</span> <span class="o">+=</span> <span class="mi">1</span><span class="p">;</span>
        <span class="k">if</span> <span class="o">*</span><span class="n">num_reporting</span> <span class="o">==</span> <span class="k">self</span><span class="py">.workers</span><span class="nf">.len</span><span class="p">()</span> <span class="p">{</span>
            <span class="c">// This frame is done, save it out to a PNG
</span>
            <span class="n">finished</span> <span class="o">=</span> <span class="k">true</span><span class="p">;</span>
        <span class="p">}</span>
    <span class="p">},</span>
    <span class="o">&amp;</span><span class="k">mut</span> <span class="nn">DistributedFrame</span><span class="p">::</span><span class="n">Completed</span> <span class="k">=&gt;</span> <span class="nd">println!</span><span class="p">(</span><span class="s">"Worker reporting on completed frame {}?"</span><span class="p">,</span> <span class="n">frame_num</span><span class="p">),</span>
<span class="p">}</span>
<span class="c">// This is a bit awkward, since we borrow df in the match we can't mark it completed in there
</span>
<span class="k">if</span> <span class="n">finished</span> <span class="p">{</span>
    <span class="o">*</span><span class="n">df</span> <span class="o">=</span> <span class="nn">DistributedFrame</span><span class="p">::</span><span class="n">Completed</span><span class="p">;</span>
<span class="p">}</span></code></pre></figure>

<p>A possible improvement here is to have the job of adding the worker’s results to the distributed frame
be managed by a threadpool, or at least off load the work of saving the image to some
other threads. This would free up more time on the event loop for the master to read more data from
the workers, currently it will be busy for some time accumulating results from workers and saving
the images.</p>

<h3 id="summary">Summary</h3>

<p>Some of the details are bit involved but overall the distributed computation is not too complicated.
The work decomposition chosen allows us to get away without any communication between the workers,
they just need to know what part of the image they’re rendering and who to send results back to. Since
we also assume that the scene data is available on each worker either through a shared file system
or by simplying being copied to each machine we don’t have to worry about sending the models and such over
either. When the
worker launches it starts listening for the master on a hard-coded port number (63234). The master
is launched and passed the worker’s hostnames or IP addresses and it
starts opening a TCP connection to each and enters mio’s event loop.</p>

<p>Once a worker is writable (we’ve connected successfully) the master sends the worker its instructions
and stops watching for writable events on the connection. Upon recieving instructions the worker begins
rendering using the desired number of threads (defaults to number of logical cores). After finishing
its blocks for a frame the worker sends its results back to the master which is watching for readable events
on all the workers. Once the master has collected results from all the workers
for a frame it saves out the image and marks it completed. After the workers have finished their blocks
for the last frame being rendered they exit, once the master has saved out the last frame it also exits.</p>

<h3 id="code">Code</h3>

<p>The full code for the distributed rendering is in tray_rust’s <a href="https://github.com/Twinklebear/tray_rust/tree/master/src/exec/distrib">exec::distrib</a>
module if you’re interested in some more details.</p>

<h1 id="scalability">Scalability</h1>

<p>Now that we’ve implemented a distributed computation we’d like to know how well it scales as we add
more workers (strong scaling). For these tests I used two scenes: one is pretty simple with some minor
work imbalance which should reveal issues more related to communication overhead while the other scene
is very imbalanced and will test scaling issues in the presence of uneven work distribution.</p>

<p>The simpler scene is the classic Cornell box which we’ll render at 800x600 resolution with
1024 samples per pixel using path tracing. The load imbalanced scene is the
<a href="http://graphics.stanford.edu/data/3Dscanrep/">Stanford Buddha</a> placed inside the Cornell box,
this results in a few workers having a
complex model to deal with while the majority of them are just intersecting the walls. The Buddha
box is rendered at 1280x720 with 1024 samples per pixel using path tracing. Since we’re using
path tracing it’s likely that paths traced which initially hit a wall will bounce around the scene
and intersect the Buddha but the pixels that see it directly will still have more work
vs. those that hit it indirectly.</p>

<div class="col-md-12 text-md-center">
<img class="img-fluid" src="https://cdn.willusher.io/img/usuLnIj.webp" alt="Cornell Box" />
<i>Cornell Box test scene</i>

<img class="img-fluid" src="https://cdn.willusher.io/img/hOZmzlB.webp" alt="Buddha Box" />
<i>Buddha Box test scene</i>
</div>

<p>To run these scaling tests I used two clusters at my lab, one is a bit older which we’ll refer to
as <em>old</em> while the newer machine we’ll call <em>new</em>. The machine specifications are:</p>

<ul>
  <li>
    <p><em>old:</em> 64 nodes, each with two <a href="http://ark.intel.com/products/37106/Intel-Xeon-Processor-X5550-8M-Cache-2_66-GHz-6_40-GTs-Intel-QPI">Xeon X5550</a> CPUs.
This is a legacy machine though so a few nodes have failed and won’t be repaired, combined with other users
running on the machine as well I was only able to get up to 44 nodes.</p>
  </li>
  <li>
    <p><em>new:</em> 32 nodes, each with two <a href="http://ark.intel.com/products/64584/Intel-Xeon-Processor-E5-2660-20M-Cache-2_20-GHz-8_00-GTs-Intel-QPI">Xeon E5-2660</a> CPUs.
Since some other users are running on
this machine (but not using many threads) I used just 30 threads/node since they had the remaining two.
This shouldn’t change the speedup numbers though since our
baseline time (1x) is a single node with 30 threads. On this machine I tested up to 28 nodes as a few were
down or otherwise occupied.</p>
  </li>
</ul>

<p>These plots show speedup over a single node which is set as our baseline of 1x compared to perfect
strong scaling. In the case of perfect strong scaling we’d expect that if 1 node is 1x then 20 nodes
should run at 20x the speed, however this is quite hard to achieve.</p>

<div class="col-md-12">
<div class="col-md-10 offset-md-1 text-md-center">
<img class="img-fluid" src="/assets/img/distrib_rendering_scaling/old_scaling.svg" />
<i>Old cluster strong scaling, 16 threads/node</i>
</div>
<div class="col-md-10 offset-md-1 text-md-center">
<img class="img-fluid" src="/assets/img/distrib_rendering_scaling/new_scaling.svg" />
<i>New cluster strong scaling, 30 threads/node</i>
</div>
</div>

<p>On the <em>old</em> cluster for the Cornell box we see a speedup of 37.57x when using 44 nodes, for
the Buddha box we see only 30.53x. On the <em>new</em> cluster
for the Cornell box we get a speedup of 24.95x at 28 nodes while with the Buddha box we get
a speedup of just 18.91x. So why do we not scale as well as we’d hope, especially on the Buddha box?
I have two ideas on possible improvements to tray_rust’s scalability.</p>

<h2 id="scaling-issue-1-grouping-workers-results">Scaling Issue 1: Grouping Worker’s Results</h2>

<p>Currently we have a pretty large chunk of overhead when sending results back: for each 2x2 block of results
sent by a worker it includes an additional 16 byte header specifying the block’s location, adding
a 25% size overhead to the block. This is most clearly seen in the Cornell box tests since its workload
is relatively balanced. We can see that it stays closer to perfect scaling for longer than the
Buddha box and when it starts to break down some (about 22 nodes on <em>old</em> and 16 on <em>new</em>) it doesn’t drop
as fast. However it clearly is trailing off further from perfect scaling
as we get to higher node counts where we get less and less speedup for each additional node.</p>

<p>I think this is coming from an increased amount of data sent to the master as we add nodes.
Recall that when using reconstruction filtering nodes write to pixels in blocks assigned to
other nodes so neighbors will send some overlapping data. The amount of overlapping data grows as
we add more nodes since more nodes will overlap each other’s regions. A possible solution is to
find the fewest number of bounding rectangles that contain the pixels the node has written to that will
minimize the amount of pixels and block headers we need to send.</p>

<p>This is mostly a strong hunch at the moment but will be cleared up by doing some scaling tests
on renders without reconstruction filtering. Without reconstruction filtering there will be no overlap
between regions that nodes write to so we can measure scaling without this issue of overlapping data.</p>

<h2 id="scaling-issue-2-work-stealing">Scaling Issue 2: Work Stealing</h2>

<p>The Buddha box scene performs even worse than the Cornell box scene, it starts to drop off at lower node
counts than the Cornell box and continues to fall behind in terms of added speedup per node. While the culprit
in the Cornell box scaling results I’m a bit less sure on, I’m pretty confident that a lack of load balancing
among the workers is what hurts the Buddha box scene’s scaling the most.</p>

<p>When looking at some of the individual worker render times
for a 25 node run on <em>new</em> the fastest worker finished in 39.2s while the slowest
took 57.6s! In fact most nodes finished in about 39-42s while the few stuck with large portions of the Buddha
took 51-57s.
For about 22s we have workers finishing and exiting when they could actually be helping other workers still
rendering to finish faster.</p>

<p>The clear fix here is to implement some form of distributed work stealing to allow workers to look around
for more blocks to render once they finish their assigned blocks (minus any stolen from them). I don’t
know anything about distributed work stealing and it sounds like a pretty complicated topic so I haven’t
thought much about actually implementing this. It would be really awesome to try out though and should help
quite a bit with scalability, especially on imbalanced scenes.</p>

<h2 id="scalability-cap">Scalability Cap</h2>

<p>As a result of the block based work decomposition we chose in the beginning we’re limited on how
many threads we can take advantage of. For the 400x304 Cornell box example we have 1900 8x8 blocks to
render, if we have more than 1900 cores available we can’t take advantage of them as we simply have
no work to assign them. An easy fix here would be to detect this case and have the master
or workers subdivide the blocks to 4x4 or 2x2 to decompose the problem further so we can
assign these extra threads some work.</p>

<h2 id="fault-tolerance">Fault Tolerance</h2>

<p>Another limitation of the current distributed rendering is that there is no fault tolerance. If a worker
goes down in the current system the master will detect the error and terminate, which will propagate the
termination to the remaining workers. For long runs especially at high node counts worker failures
are not that rare and a much better approach here would be to redistribute the failed worker’s blocks.
I’m unsure if this is something I’ll implement, in the presence of work stealing it would become even more
complex. For example we wouldn’t want to re-assign blocks that were stolen from the worker before it crashed,
but determining which blocks were stolen might be tough.</p>

<h1 id="try-tray_rust-yourself">Try tray_rust Yourself!</h1>

<p>Recently I’ve put together a simple <a href="https://github.com/Twinklebear/tray_rust_blender">Blender plugin</a>
for tray_rust which will let you export static and keyframe animated scenes from Blender to a tray_rust
scene. There are still quite a few limitations which you can see discussed on the Github page but most
of these aren’t as difficult to work around compared to positioning objects by hand in a text file.
You’ll still need to specify materials by hand in the scene file, this is documented in the
<a href="http://www.willusher.io/tray_rust/tray_rust/material/index.html">materials</a> module but is still pretty
user unfriendly.
Definitely try it out, if you put together a scene I’d be really excited to see it so tweet any
renders to me on Twitter <a href="https://twitter.com/_wusher">@_wusher</a>!</p>

<p>Here’s a neat one I put together using Blender’s physics simulation by baking the simulation
to keyframes before exporting.  The Rust logo was
modeled by <a href="http://blenderartists.org/forum/showthread.php?362836-Rust-language-3D-logo">Nylithius</a>, many
of the models in the scene make use of measured material data from the
<a href="http://www.merl.com/brdf/">MERL BRDF database</a>. The animation was rendered with 28 nodes on <em>new</em>
using the distributed renderer with 1024 samples per pixel and took 03:44:02 to render.</p>

<div class="embed-responsive embed-responsive-16by9">
<iframe class="embed-responsive-item" src="https://www.youtube-nocookie.com/embed/_fM9T-z3kik" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe>
</div>

<p>If you’d like to try using the distributed renderer you can do so with any machines on a network, so
some desktops or laptops will work, or you can grab some compute instances from Google Compute Engine
or AWS EC2.
See the <a href="http://www.willusher.io/tray_rust/tray_rust/exec/distrib/index.html">exec::distrib</a>
module documentation for more information on how to run the distributed render,
or run tray_rust with <code class="highlighter-rouge">-h</code> for a shorter summary of options.</p>

<script src="/assets/markdeep_modified_min.js"></script>


		<hr>
		<div class="col-12 row">
			
			<div class="col-md-6 justify-content-left">
				<a href="/2015/12/16/rendering-an-animation-in-rust" title="Rendering an Animation in Rust">
					<span class="fa fa-chevron-left" aria-hidden="true"></span>&nbsp Previous</a>
			</div>
			
			
			<div class="col-6 text-right">
				<a href="/latex/2018/07/10/comments-in-latex" title="Comments in LaTeX">
					Next &nbsp<span class="fa fa-chevron-right" aria-hidden="true"></span></a>
			</div>
			
		</div>
	</div>
</div>



	</div>

    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.12.9/umd/popper.min.js" integrity="sha384-ApNbgh9B+Y1QKtv3Rn7W3mgPxhU9K/ScQsAP7hUibX39j7fakFPskvXusvfa0b4Q" crossorigin="anonymous"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/js/bootstrap.min.js" integrity="sha384-JZR6Spejh4U02d8jOt6vLEHfe/JQGiRRSQQxSfFWpi1MquVdAyjUar5+76PVCmYl" crossorigin="anonymous"></script>
    

    <script defer data-domain="willusher.io" src="https://pan.wushernet.com/js/script.js"></script>

</body>
</html>



