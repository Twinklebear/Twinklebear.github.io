

<!DOCTYPE html>
<html lang="en">
<head>
	<meta charset="utf-8">
	<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

	<title>Volume Rendering with WebGL</title>
	
		<meta name="description" content="">
	
    <meta name="author" content="Will Usher">

    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css" integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">

	<!-- Should I be including font-awesome from bootstrap? Or just the 5.0 version? -->
    <!--<script defer src="https://use.fontawesome.com/releases/v5.0.6/js/all.js"></script>-->
    <script defer src="https://kit.fontawesome.com/b56adde3a3.js" crossorigin="anonymous"></script>
    <script src="https://code.jquery.com/jquery-3.2.1.slim.min.js" integrity="sha384-KJ3o2DKtIkvYIK3UENzmM7KCkRr/rE9/Qpg6aAZGJwFDMVNA/GpGFF93hXpG5KkN" crossorigin="anonymous"></script>

	<link href="/assets/molokai.css" rel="stylesheet">
	<link href="/assets/custom.css" rel="stylesheet">
	<link rel="shortcut icon" href="/assets/img/identicon.ico">

</head>
<body>
	<nav class="navbar navbar-expand-md navbar-dark bg-dark static-top">
		<div class="container">
			<a class="navbar-brand" href="/">Will Usher</a>
			<button class="navbar-toggler navbar-toggler-right" type="button" data-toggle="collapse"
				data-target="#navbar_responsive" aria-controls="navbarResponsive" aria-expanded="false"
				aria-label="Toggle navigation">
					<span class="navbar-toggler-icon"></span>
			</button>
			<div class="collapse navbar-collapse" id="navbar_responsive">
				<ul class="navbar-nav mr-auto">
				
				
				  
					
					  
					  
						<li class="nav-item"><a href="/projects" class="nav-link">
							Projects</a></li>
						
					  
					
				  
					
					  
					  
						<li class="nav-item"><a href="/blog" class="nav-link">
							Blog</a></li>
						
					  
					
				  
					
					  
					  
						<li class="nav-item"><a href="/pages/sdl2" class="nav-link">
							SDL2 Tutorials</a></li>
						
					  
					
				  
					
					  
					  
						<li class="nav-item"><a href="/news" class="nav-link">
							News</a></li>
						
					  
					
				  
					
					  
					  
						<li class="nav-item"><a href="/archive" class="nav-link">
							Archive</a></li>
						
					  
					
				  
				</ul>
			</div><!-- nav.collaps -->
		</div>
	</nav>

	<div class="container content mb-4">
		

<h1 class="display-4">Volume Rendering with WebGL</h1>

<div class="row justify-content-center">
	<div class="col-9">
        <p>January 13, 2019</p>
		
<figure>
	<img class="img-fluid" src="https://i.imgur.com/YqdyKCj.png" />
	
	<figcaption><i>Figure 1:
	Example volume renderings, using the WebGL volume renderer described in this post.
	Left: A simulation of the spatial probability distribution
	of electrons in a high potential protein molecule.
	Right: A CT scan of a Bonsai Tree.
	Both datasets are from the
	<a href="https://klacansky.com/open-scivis-datasets/">Open SciVis Datasets</a>
	repository.
	</i></figcaption>
</figure>

<p>In scientific visualization, volume rendering is widely used to visualize
3D scalar fields. These scalar fields are often
uniform grids of values, representing,
for example, charge density around a molecule,
an MRI or CT scan, air flow around an airplane, etc.
Volume rendering is a conceptually straightforward method
for turning such data into an image: by sampling the data
along rays from the eye and assigning
a color and transparency to each sample, we can
produce useful and beautiful images of such scalar fields
(see Figure 1).
In a GPU renderer, these 3D scalar fields are stored
as 3D textures; however, in WebGL1 3D textures were
not supported, requiring additional hacks to emulate them
for volume rendering.
Recently, WebGL2 added support for 3D textures,
allowing for an elegant and fast volume renderer to be
implemented entirely in the browser.
In this post we’ll discuss the mathematical background
for volume rendering, and how it can be implemented in
WebGL2 to create an interactive volume renderer
entirely in the browser!
Before we start, you can try out the volume renderer
described in this post <a href="https://www.willusher.io/webgl-volume-raycaster/">online</a>.</p>

<!--more-->

<h1 id="1-introduction">1. Introduction</h1>

<figure>
	<img class="img-fluid" width="80%" src="/assets/img/webgl-volumes/volume-rendering-cloud.svg" />
	
	<figcaption><i>Figure 2:
	Physically based volume rendering, accounting for absorption and
	light emission by the volume, along with scattering effects.
	</i></figcaption>
</figure>

<p>To produce a physically realistic image from volumetric data, we need to model how light rays
are absorbed, emitted, and scattered by the medium (Figure 2).
While modelling light transport through a medium at this level
produces beautiful, physically correct images, it is too expensive for
interactive rendering, which is our goal in visualization software.
In scientific visualization, our end goal is
to allow scientists to interactively explore their data, enabling them
to ask and answer questions about their research problem.
As a complete physically based model with scattering is too expensive
for interactive rendering, visualization applications
employ a simplified emission-absorption model, either ignoring expensive
scattering effects, or approximating them in some way.
Here we will just focus on the emission-absorption model.</p>

<p>In the emission-absorption model, we only compute lighting effects occurring
directly along the black ray in Figure 2, and ignore those from the dashed gray
rays. Rays passing through the volume and reaching the eye accumulate color emitted by the
volume, and are attenuated as they traverse it due to absorption by the volume.
If we trace rays from the eye through the volume, we can
compute they light received at the eye by integrating the ray through
the volume, to accumulate the emission and absorption along the ray.
Given a ray from the eye which enters the volume at <script type="math/tex">s = 0</script> and exits it at
<script type="math/tex">s = L</script>, we can compute the light which
is received at the eye using the following integral:</p>

<script type="math/tex; mode=display">C(r) = \int_0^L C(s) \mu(s) e^{-\int_0^s \mu(t) dt} ds</script>

<p>As the ray passes through the volume, we integrate
the emitted color <script type="math/tex">C(s)</script> and absorption <script type="math/tex">\mu(s)</script>
at each point <script type="math/tex">s</script> along the ray. The emitted color at each point is attenuated as it returns
to the eye by the volume’s absorption up to that point, which is computed with the
<script type="math/tex">e^{-\int_0^s \mu(t) dt}</script> term.</p>

<p>In general, this integral cannot be computed analytically, and
we must use a numeric approximation. We approximate the integral by taking a set of <script type="math/tex">N</script>
samples along the ray on the interval <script type="math/tex">s = [0, L]</script>, each a distance <script type="math/tex">\Delta s</script> apart
(Figure 3), and summing these samples together. The attenuation term at
each sample point becomes a product series, accumulating the absorption at
previous samples.</p>

<script type="math/tex; mode=display">C(r) = \sum_{i=0}^N C(i \Delta s) \mu (i \Delta s) \Delta s
			\prod_{j=0}^{i-1} e^{-\mu(j \Delta s) \Delta s}</script>

<p>To simplify this sum further, we approximate the
previous attenuation term (<script type="math/tex">e^{-\mu(j \Delta s) \Delta s}</script>)
by its Taylor series. We also introduce alpha
<script type="math/tex">\alpha(i \Delta s) = \mu(i \Delta s) \Delta s</script>
for convenience. This yields the front-to-back alpha compositing equation:</p>

<script type="math/tex; mode=display">C(r) = \sum_{i=0}^N C(i \Delta s) \alpha (i \Delta s)
			\prod_{j=0}^{i-1} (1 - \alpha(j \Delta s))</script>

<figure>
	<img class="img-fluid" width="80%" src="/assets/img/webgl-volumes/volume-rendering-cloud-labelled.svg" />
	
	<figcaption><i>Figure 3:
	Computing the emission-absorption rendering integral on a volume.
	</i></figcaption>
</figure>

<p>The above equation amounts to a for loop, where we step the ray through
the volume, and accumulate the color and opacity iteratively as we go.
This loop continues until the ray either leaves the volume,
or the accumulated color has become opaque (<script type="math/tex">\alpha = 1</script>).
The iterative computation of the above sum is done using the familiar
front-to-back compositing equations:</p>

<script type="math/tex; mode=display">\hat{C}_i = \hat{C}_{i-1} + (1 - \alpha_{i-1}) \hat{C}(i \Delta s)</script>

<script type="math/tex; mode=display">\alpha_i = \alpha_{i - 1} + (1 - \alpha_{i-1}) \alpha(i \Delta s)</script>

<p>In these final equations we use pre-multiplied opacity for
correct blending, <script type="math/tex">\hat{C}(i\Delta s) = C(i\Delta s) \alpha(i \Delta s)</script>.</p>

<p>To render an image of the volume we just need to trace a ray
from the eye through each pixel, and perform the above
iteration for each ray intersecting the volume.
Each ray (or pixel) we process is independent, so if we want
to render the image quickly all we need is a way to process
a large number of pixels in parallel.
This is where the GPU comes in.
By implementing the raymarching process in a fragment shader
we can leverage the parallel computing power of the GPU to
implement a very fast volume renderer!</p>

<!--
**Todo: mention that the volume represents a continous field,
so when we sample along the grid we'll do trilinear interpolation to
get the sample value**
<figure>
	<img class="img-fluid" width="70%"
		src="/assets/img/webgl-volumes/raymarching-grid.svg"/>
	
	<figcaption><i>Figure 4:
	Raymarching the volume grid.
	</i></figcaption>
</figure>
-->

<h1 id="2-gpu-implementation-with-webgl2">2. GPU Implementation with WebGL2</h1>

<p>To run our raymarching work in the fragment shader, we need
to get the GPU to run the fragment shader for the pixels we want
to trace rays through.
However, the OpenGL pipeline works on geometric primitives (Figure 5),
and does not have a direct method to run the fragment shader on
some region of the screen.
To work around this, we can render some proxy geometry to execute
the fragment shader on the pixels we want to render.
Our approach to rendering the volume will be
similar to those of <a href="https://www.shadertoy.com/">Shader Toy</a>
and <a href="https://iquilezles.org/www/material/nvscene2008/nvscene2008.htm">demoscene renderers</a>,
which render two full-screen triangles to execute the fragment
shader, which then does the real rendering work.</p>

<figure>
	<img class="img-fluid" src="/assets/img/webgl-volumes/webgl-triangle-pipeline.svg" />
	
	<figcaption><i>Figure 5:
	The OpenGL pipeline in WebGL consists of two programmable shader stages:
	the vertex shader, responsible for transforming input
	vertices into clip space, and the fragment shader, responsible
	for shading pixels covered by triangle.
	</i></figcaption>
</figure>

<p>While rendering two full-screen triangles as in ShaderToy will work, it would run
an unnecessary amount of fragment processing in the case that the
volume does not cover the entire screen. This case is actually
quite common, as users zoom out to get an overview of the dataset or study
large-scale features. To restrict the fragment processing work
to just those pixels touched by the volume, we can rasterize
the bounding box of the volume grid, and then run the raymarching
step in the fragment shader.
Finally, we don’t want to render
both the front and back faces of the box, as this could run our
fragment shader twice, depending on the order the triangles are rendered in.
Furthermore, if we render just the front faces we’ll run into issues
when the user zooms in to the volume, as the front faces will
project behind the camera and be clipped, causing those pixels
to not be rendered. To allow users to
zoom fully into the volume, we’ll render just the back faces of
the box. Our resulting rendering pipeline is shown in Figure 6.</p>

<figure>
	<img class="img-fluid" src="/assets/img/webgl-volumes/webgl-volume-raycast-pipeline.svg" />
	
	<figcaption><i>Figure 6:
	The WebGL pipeline for raymarching a volume. We rasterize
	the backfaces of the volume's bounding box to run
	the fragment shader work for those pixels touched by the volume.
	Within the fragment shader we step rays through the volume
	to render it.
	</i></figcaption>
</figure>

<p>In this pipeline, the bulk of real rendering work is done in the
fragment shader; however, we can still use the vertex shader and
the fixed function interpolation hardware for some useful computation.
Our vertex shader will transform the volume based on the user’s camera
position, and compute the ray direction and eye position
in the volume space, and pass them to the fragment shader.
The ray direction computed at each vertex is then interpolated
across the triangle for us by the fixed function interpolation
hardware on the GPU, letting us compute the ray directions for
each fragment a bit cheaper. However, these directions
may not be normalized when we get them in the fragment
shader, so we’ll still need to normalize them.</p>

<p>We’ll render the bounding box as a unit <script type="math/tex">[0, 1]</script>
cube, and scale it by the volume axes to support non-uniform sized
volumes. The eye position is transformed into the unit cube,
and the ray direction is computed in this space. Raymarching in
the unit cube space will allow us to simplify our texture sampling
operations during the raymarching in the fragment shader, since we’ll already be in the
<script type="math/tex">[0, 1]</script> texture coordinate space of the 3D volume.</p>

<p>The vertex shader we’ll use is shown below, the rasterized
back faces colored by the view ray direction are shown in Figure 7.</p>

<figure class="highlight"><pre><code class="language-glsl" data-lang="glsl"><span class="cp">#version 300 es
</span><span class="k">layout</span><span class="p">(</span><span class="n">location</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="k">in</span> <span class="kt">vec3</span> <span class="n">pos</span><span class="p">;</span>
<span class="k">uniform</span> <span class="kt">mat4</span> <span class="n">proj_view</span><span class="p">;</span>
<span class="k">uniform</span> <span class="kt">vec3</span> <span class="n">eye_pos</span><span class="p">;</span>
<span class="k">uniform</span> <span class="kt">vec3</span> <span class="n">volume_scale</span><span class="p">;</span>

<span class="k">out</span> <span class="kt">vec3</span> <span class="n">vray_dir</span><span class="p">;</span>
<span class="k">flat</span> <span class="k">out</span> <span class="kt">vec3</span> <span class="n">transformed_eye</span><span class="p">;</span>

<span class="kt">void</span> <span class="nf">main</span><span class="p">(</span><span class="kt">void</span><span class="p">)</span> <span class="p">{</span>
	<span class="c1">// Translate the cube to center it at the origin.
</span>
	<span class="kt">vec3</span> <span class="n">volume_translation</span> <span class="o">=</span> <span class="kt">vec3</span><span class="p">(</span><span class="mi">0</span><span class="p">.</span><span class="mi">5</span><span class="p">)</span> <span class="o">-</span> <span class="n">volume_scale</span> <span class="o">*</span> <span class="mi">0</span><span class="p">.</span><span class="mi">5</span><span class="p">;</span>
	<span class="nb">gl_Position</span> <span class="o">=</span> <span class="n">proj_view</span> <span class="o">*</span> <span class="kt">vec4</span><span class="p">(</span><span class="n">pos</span> <span class="o">*</span> <span class="n">volume_scale</span> <span class="o">+</span> <span class="n">volume_translation</span><span class="p">,</span> <span class="mi">1</span><span class="p">);</span>

	<span class="c1">// Compute eye position and ray directions in the unit cube space
</span>
	<span class="n">transformed_eye</span> <span class="o">=</span> <span class="p">(</span><span class="n">eye_pos</span> <span class="o">-</span> <span class="n">volume_translation</span><span class="p">)</span> <span class="o">/</span> <span class="n">volume_scale</span><span class="p">;</span>
	<span class="n">vray_dir</span> <span class="o">=</span> <span class="n">pos</span> <span class="o">-</span> <span class="n">transformed_eye</span><span class="p">;</span>
<span class="p">};</span></code></pre></figure>

<figure>
	<img class="img-fluid" width="70%" src="https://i.imgur.com/FMWE7UR.png" />
	
	<figcaption><i>Figure 7:
	The volume bounding box back faces, colored by ray direction.
	</i></figcaption>
</figure>

<p>Now that we have our fragment shader running on the pixels that we need to
render the volume for, we can raymarch the volume and compute a color
for each pixel. In addition to the ray direction and eye position we computed
in the vertex shader, we’ll need a few more inputs to the fragment shader
to render the volume. Of course, the first thing we’ll need is a 3D texture
sampler to sample the volume.
However, the volume is just a block of scalar values,
if we used these scalars directly as the color (<script type="math/tex">C(s)</script>) and opacity (<script type="math/tex">\alpha(s)</script>) values,
the rendered grayscale image may not be very useful to the user. For example,
it would not be possible to highlight regions of interest with different colors,
or to make noise and background regions transparent to hide them.</p>

<p>To give the user control over the color and
opacity assigned to each sample value, scientific visualization renderers use an
additional colormap, called a <em>Transfer Function</em>.
The transfer function specifies what color and opacity value should
be assigned for a given value sampled from the volume.
Although more complex
transfer functions exist, a typical one acts as a simple color lookup table,
and can be represented as a 1D color and opacity texture (RGBA). To apply the transfer
function when raymarching the volume, we can sample the
transfer function’s texture using the scalar value sampled from the volume texture.
The returned color and opacity values are then used as <script type="math/tex">C(s)</script> and
<script type="math/tex">\alpha(s)</script> for the sample.</p>

<p>The final additional input we need for our fragment shader are the volume dimensions,
which we’ll use to compute a ray step size (<script type="math/tex">\Delta s</script>) to sample
each voxel along the ray at least once. As the <a href="http://www.pbr-book.org/3ed-2018/Geometry_and_Transformations/Rays.html">conventional equation for a ray</a>
is <script type="math/tex">r(t) = \vec{o} + t \vec{d}</script>, we will switch the terminology in the code to match,
and refer to <script type="math/tex">\Delta s</script> as <script type="math/tex">\texttt{dt}</script>. Similarly, the range along the ray overlapped by
the volume, <script type="math/tex">s = [0, L]</script>, will be referred to as <script type="math/tex">[\texttt{tmin}, \texttt{tmax}]</script>.</p>

<p>To raymarch the volume in our fragment shader, we will do the following:</p>

<ol>
  <li>Normalize the view ray direction received as input from the vertex shader;</li>
  <li>Intersect the view ray against the volume bounds to determine the interval <script type="math/tex">[\texttt{tmin}, \texttt{tmax}]</script>
 to raymarch over to render the volume;</li>
  <li>Compute the step size <script type="math/tex">\texttt{dt}</script> such that each voxel is sampled at least once;</li>
  <li>Starting from the entry point at <script type="math/tex">r(\texttt{tmin})</script>, step the ray through the volume until
 the exit point at <script type="math/tex">r(\texttt{tmax})</script>.
    <ol>
      <li>At each point, sample the volume and use the transfer function to assign a color
 and opacity;</li>
      <li>Accumulate the color and opacity along the ray using the front-to-back compositing equation.</li>
    </ol>
  </li>
</ol>

<p>As an additional optimization, we can include an early exit condition in our
raymarching loop to break out when the accumulated color is nearly opaque.
Once the color is nearly opaque, any samples after will have little to no
contribution to the pixel, as their color is fully absorbed by the medium
before reaching the eye.</p>

<p>The full fragment shader for our volume renderer is shown below, with
additional comments marking each step in the process.</p>

<figure class="highlight"><pre><code class="language-glsl" data-lang="glsl"><span class="cp">#version 300 es
</span><span class="k">precision</span> <span class="kt">highp</span> <span class="kt">int</span><span class="p">;</span>
<span class="k">precision</span> <span class="kt">highp</span> <span class="kt">float</span><span class="p">;</span>

<span class="k">uniform</span> <span class="kt">highp</span> <span class="kt">sampler3D</span> <span class="n">volume</span><span class="p">;</span>
<span class="c1">// WebGL doesn't support 1D textures, so we use a 2D texture for the transfer function
</span>
<span class="k">uniform</span> <span class="kt">highp</span> <span class="kt">sampler2D</span> <span class="n">transfer_fcn</span><span class="p">;</span>
<span class="k">uniform</span> <span class="kt">ivec3</span> <span class="n">volume_dims</span><span class="p">;</span>

<span class="k">in</span> <span class="kt">vec3</span> <span class="n">vray_dir</span><span class="p">;</span>
<span class="k">flat</span> <span class="k">in</span> <span class="kt">vec3</span> <span class="n">transformed_eye</span><span class="p">;</span>

<span class="k">out</span> <span class="kt">vec4</span> <span class="n">color</span><span class="p">;</span>

<span class="kt">vec2</span> <span class="nf">intersect_box</span><span class="p">(</span><span class="kt">vec3</span> <span class="n">orig</span><span class="p">,</span> <span class="kt">vec3</span> <span class="n">dir</span><span class="p">)</span> <span class="p">{</span>
	<span class="k">const</span> <span class="kt">vec3</span> <span class="n">box_min</span> <span class="o">=</span> <span class="kt">vec3</span><span class="p">(</span><span class="mi">0</span><span class="p">);</span>
	<span class="k">const</span> <span class="kt">vec3</span> <span class="n">box_max</span> <span class="o">=</span> <span class="kt">vec3</span><span class="p">(</span><span class="mi">1</span><span class="p">);</span>
	<span class="kt">vec3</span> <span class="n">inv_dir</span> <span class="o">=</span> <span class="mi">1</span><span class="p">.</span><span class="mi">0</span> <span class="o">/</span> <span class="n">dir</span><span class="p">;</span>
	<span class="kt">vec3</span> <span class="n">tmin_tmp</span> <span class="o">=</span> <span class="p">(</span><span class="n">box_min</span> <span class="o">-</span> <span class="n">orig</span><span class="p">)</span> <span class="o">*</span> <span class="n">inv_dir</span><span class="p">;</span>
	<span class="kt">vec3</span> <span class="n">tmax_tmp</span> <span class="o">=</span> <span class="p">(</span><span class="n">box_max</span> <span class="o">-</span> <span class="n">orig</span><span class="p">)</span> <span class="o">*</span> <span class="n">inv_dir</span><span class="p">;</span>
	<span class="kt">vec3</span> <span class="n">tmin</span> <span class="o">=</span> <span class="n">min</span><span class="p">(</span><span class="n">tmin_tmp</span><span class="p">,</span> <span class="n">tmax_tmp</span><span class="p">);</span>
	<span class="kt">vec3</span> <span class="n">tmax</span> <span class="o">=</span> <span class="n">max</span><span class="p">(</span><span class="n">tmin_tmp</span><span class="p">,</span> <span class="n">tmax_tmp</span><span class="p">);</span>
	<span class="kt">float</span> <span class="n">t0</span> <span class="o">=</span> <span class="n">max</span><span class="p">(</span><span class="n">tmin</span><span class="p">.</span><span class="n">x</span><span class="p">,</span> <span class="n">max</span><span class="p">(</span><span class="n">tmin</span><span class="p">.</span><span class="n">y</span><span class="p">,</span> <span class="n">tmin</span><span class="p">.</span><span class="n">z</span><span class="p">));</span>
	<span class="kt">float</span> <span class="n">t1</span> <span class="o">=</span> <span class="n">min</span><span class="p">(</span><span class="n">tmax</span><span class="p">.</span><span class="n">x</span><span class="p">,</span> <span class="n">min</span><span class="p">(</span><span class="n">tmax</span><span class="p">.</span><span class="n">y</span><span class="p">,</span> <span class="n">tmax</span><span class="p">.</span><span class="n">z</span><span class="p">));</span>
	<span class="k">return</span> <span class="kt">vec2</span><span class="p">(</span><span class="n">t0</span><span class="p">,</span> <span class="n">t1</span><span class="p">);</span>
<span class="p">}</span>

<span class="kt">void</span> <span class="nf">main</span><span class="p">(</span><span class="kt">void</span><span class="p">)</span> <span class="p">{</span>
	<span class="c1">// Step 1: Normalize the view ray
</span>
	<span class="kt">vec3</span> <span class="n">ray_dir</span> <span class="o">=</span> <span class="n">normalize</span><span class="p">(</span><span class="n">vray_dir</span><span class="p">);</span>

	<span class="c1">// Step 2: Intersect the ray with the volume bounds to find the interval
</span>
	<span class="c1">// along the ray overlapped by the volume.
</span>
	<span class="kt">vec2</span> <span class="n">t_hit</span> <span class="o">=</span> <span class="n">intersect_box</span><span class="p">(</span><span class="n">transformed_eye</span><span class="p">,</span> <span class="n">ray_dir</span><span class="p">);</span>
	<span class="k">if</span> <span class="p">(</span><span class="n">t_hit</span><span class="p">.</span><span class="n">x</span> <span class="o">&gt;</span> <span class="n">t_hit</span><span class="p">.</span><span class="n">y</span><span class="p">)</span> <span class="p">{</span>
		<span class="k">discard</span><span class="p">;</span>
	<span class="p">}</span>
	<span class="c1">// We don't want to sample voxels behind the eye if it's
</span>
	<span class="c1">// inside the volume, so keep the starting point at or in front
</span>
	<span class="c1">// of the eye
</span>
	<span class="n">t_hit</span><span class="p">.</span><span class="n">x</span> <span class="o">=</span> <span class="n">max</span><span class="p">(</span><span class="n">t_hit</span><span class="p">.</span><span class="n">x</span><span class="p">,</span> <span class="mi">0</span><span class="p">.</span><span class="mi">0</span><span class="p">);</span>

	<span class="c1">// Step 3: Compute the step size to march through the volume grid
</span>
	<span class="kt">vec3</span> <span class="n">dt_vec</span> <span class="o">=</span> <span class="mi">1</span><span class="p">.</span><span class="mi">0</span> <span class="o">/</span> <span class="p">(</span><span class="kt">vec3</span><span class="p">(</span><span class="n">volume_dims</span><span class="p">)</span> <span class="o">*</span> <span class="n">abs</span><span class="p">(</span><span class="n">ray_dir</span><span class="p">));</span>
	<span class="kt">float</span> <span class="n">dt</span> <span class="o">=</span> <span class="n">min</span><span class="p">(</span><span class="n">dt_vec</span><span class="p">.</span><span class="n">x</span><span class="p">,</span> <span class="n">min</span><span class="p">(</span><span class="n">dt_vec</span><span class="p">.</span><span class="n">y</span><span class="p">,</span> <span class="n">dt_vec</span><span class="p">.</span><span class="n">z</span><span class="p">));</span>

	<span class="c1">// Step 4: Starting from the entry point, march the ray through the volume
</span>
	<span class="c1">// and sample it
</span>
	<span class="kt">vec3</span> <span class="n">p</span> <span class="o">=</span> <span class="n">transformed_eye</span> <span class="o">+</span> <span class="n">t_hit</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">ray_dir</span><span class="p">;</span>
	<span class="k">for</span> <span class="p">(</span><span class="kt">float</span> <span class="n">t</span> <span class="o">=</span> <span class="n">t_hit</span><span class="p">.</span><span class="n">x</span><span class="p">;</span> <span class="n">t</span> <span class="o">&lt;</span> <span class="n">t_hit</span><span class="p">.</span><span class="n">y</span><span class="p">;</span> <span class="n">t</span> <span class="o">+=</span> <span class="n">dt</span><span class="p">)</span> <span class="p">{</span>
		<span class="c1">// Step 4.1: Sample the volume, and color it by the transfer function.
</span>
		<span class="c1">// Note that here we don't use the opacity from the transfer function,
</span>
		<span class="c1">// and just use the sample value as the opacity
</span>
		<span class="kt">float</span> <span class="n">val</span> <span class="o">=</span> <span class="n">texture</span><span class="p">(</span><span class="n">volume</span><span class="p">,</span> <span class="n">p</span><span class="p">).</span><span class="n">r</span><span class="p">;</span>
		<span class="kt">vec4</span> <span class="n">val_color</span> <span class="o">=</span> <span class="kt">vec4</span><span class="p">(</span><span class="n">texture</span><span class="p">(</span><span class="n">transfer_fcn</span><span class="p">,</span> <span class="kt">vec2</span><span class="p">(</span><span class="n">val</span><span class="p">,</span> <span class="mi">0</span><span class="p">.</span><span class="mi">5</span><span class="p">)).</span><span class="n">rgb</span><span class="p">,</span> <span class="n">val</span><span class="p">);</span>

		<span class="c1">// Step 4.2: Accumulate the color and opacity using the front-to-back
</span>
		<span class="c1">// compositing equation
</span>
		<span class="n">color</span><span class="p">.</span><span class="n">rgb</span> <span class="o">+=</span> <span class="p">(</span><span class="mi">1</span><span class="p">.</span><span class="mi">0</span> <span class="o">-</span> <span class="n">color</span><span class="p">.</span><span class="n">a</span><span class="p">)</span> <span class="o">*</span> <span class="n">val_color</span><span class="p">.</span><span class="n">a</span> <span class="o">*</span> <span class="n">val_color</span><span class="p">.</span><span class="n">rgb</span><span class="p">;</span>
		<span class="n">color</span><span class="p">.</span><span class="n">a</span> <span class="o">+=</span> <span class="p">(</span><span class="mi">1</span><span class="p">.</span><span class="mi">0</span> <span class="o">-</span> <span class="n">color</span><span class="p">.</span><span class="n">a</span><span class="p">)</span> <span class="o">*</span> <span class="n">val_color</span><span class="p">.</span><span class="n">a</span><span class="p">;</span>

		<span class="c1">// Optimization: break out of the loop when the color is near opaque
</span>
		<span class="k">if</span> <span class="p">(</span><span class="n">color</span><span class="p">.</span><span class="n">a</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">.</span><span class="mi">95</span><span class="p">)</span> <span class="p">{</span>
			<span class="k">break</span><span class="p">;</span>
		<span class="p">}</span>
		<span class="n">p</span> <span class="o">+=</span> <span class="n">ray_dir</span> <span class="o">*</span> <span class="n">dt</span><span class="p">;</span>
	<span class="p">}</span>
<span class="p">}</span></code></pre></figure>

<figure>
	<img class="img-fluid" width="80%" src="https://i.imgur.com/vtZqe4m.png" />
	
	<figcaption><i>Figure 8:
	The final rendered result on the Bonsai, from the same
	viewpoint as in Figure 7.
	</i></figcaption>
</figure>

<p>That’s it!
The renderer described in this post will be able to create images like the one shown in Figure 8,
and those in Figure 1. You can also try it out <a href="https://www.willusher.io/webgl-volume-raycaster/#Bonsai">online</a>.
For brevity I’ve omitted the Javascript code needed to setup a WebGL context,
upload the volume and transfer function
textures, setup the shaders, and render the cube to actually render the volume;
the full code for the renderer is available on <a href="https://github.com/Twinklebear/webgl-volume-raycaster">Github</a>
for reference.
If you have any questions about this post, feel free to get in touch via <a href="https://twitter.com/_wusher">Twitter</a>.</p>


		<hr>
		<div class="col-12 row">
			
			<div class="col-md-6 justify-content-left">
				<a href="/latex/2018/07/10/comments-in-latex" title="Comments in LaTeX">
					<span class="fa fa-chevron-left" aria-hidden="true"></span>&nbsp Previous</a>
			</div>
			
			
			<div class="col-6 text-right">
				<a href="/graphics/2019/09/06/faster-shadow-rays-on-rtx" title="Faster Shadow Rays on RTX">
					Next &nbsp<span class="fa fa-chevron-right" aria-hidden="true"></span></a>
			</div>
			
		</div>
	</div>
</div>



	</div>

    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.12.9/umd/popper.min.js" integrity="sha384-ApNbgh9B+Y1QKtv3Rn7W3mgPxhU9K/ScQsAP7hUibX39j7fakFPskvXusvfa0b4Q" crossorigin="anonymous"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/js/bootstrap.min.js" integrity="sha384-JZR6Spejh4U02d8jOt6vLEHfe/JQGiRRSQQxSfFWpi1MquVdAyjUar5+76PVCmYl" crossorigin="anonymous"></script>
    
		<script type="text/javascript" async
			src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML">
		</script>
	

</body>
</html>



