<?xml version="1.0" encoding="UTF-8" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
	<channel>
		<atom:link href="https://www.willusher.io//rss.xml" rel="self" type="application/rss+xml" />
		<title>Will Usher's Blog</title>
		<description>Computer Graphics and Programming</description>
		<link>https://www.willusher.io/</link>
		<lastBuildDate>Sat, 23 Nov 2019 21:43:07 -0800</lastBuildDate>
		<pubDate>Sat, 23 Nov 2019 21:43:07 -0800</pubDate>
		<ttl>60</ttl>
		
		<item>
			<title>The RTX Shader Binding Table Three Ways</title>
			<description>
&lt;p&gt;DirectX Ray Tracing, Vulkan’s NV Ray Tracing extension, and OptiX (or collectively, the RTX APIs)
build on the same execution model for running user code to trace
and process rays. The user creates a &lt;em&gt;Shader Binding Table&lt;/em&gt; (SBT), which consists of a set
of shader function handles and embedded parameters for these functions. The shaders in the table
are executed depending on whether or not a geometry was hit by a ray, and which geometry was hit.
When a geometry is hit, a set of parameters specified on both the host and
device side of the application combine to determine which shader is executed.
The RTX APIs provide a great deal of flexibility in how the SBT can be set up and
indexed into during rendering, leaving a number of options open to applications.
However, with incorrect SBT access leading
to crashes and difficult bugs, sparse examples or documentation, and
subtle differences in naming and SBT setup between the APIs, properly setting up
and accessing the SBT is an especially thorny part of the RTX APIs for new users.&lt;/p&gt;

&lt;p&gt;In this post we’ll look at the similarities and differences of each ray tracing API’s shader
binding table to gain a fundamental understanding of the execution model. I’ll then
present an interactive tool for constructing the SBT, building a scene which uses it,
and executing trace calls on the scene to see which hit groups and miss shaders are called.
Finally, we’ll look at how this model can be brought back to the CPU using Embree,
to potentially build a unified low-level API for ray tracing.&lt;/p&gt;


				&lt;a href=&quot;https://www.willusher.io//graphics/2019/11/20/the-sbt-three-ways&quot;&gt;Continue Reading&lt;/a&gt;
			</description>
			<link>https://www.willusher.io//graphics/2019/11/20/the-sbt-three-ways</link>
			<guid>https://www.willusher.io//graphics/2019/11/20/the-sbt-three-ways</guid>
			<pubDate>Wed, 20 Nov 2019 00:00:00 -0800</pubDate>
		</item>
		
		<item>
			<title>Faster Shadow Rays on RTX</title>
			<description>
&lt;p&gt;To determine if a hit point can be directly lit by a light source in the scene
we need to perform a visibility test between the point and the light.
In a path tracer we must perform at least one visibility test per hit point
to shade the point, or two if we’re using multiple importance sampling (one for the light
sample, and one for the BSDF sample). When rendering just ambient occlusion,
e.g., for baking occlusion maps, we may send even more shadow rays per hit-point.
Fortunately, shadow rays can be relatively cheap to trace, as
we don’t care about finding the closest hit point or computing surface shading information,
but just whether or not something is intersected by the ray.
There are a few options and combinations of ray flags which we can use
when deciding how to trace shadow rays on RTX (through DXR, OptiX or Vulkan).
I recently learned a method for skipping all hit group shaders (any hit, closest hit)
and instead using just the miss shader to determine if the ray is &lt;em&gt;not&lt;/em&gt; occluded.
This was a bit non-obvious to me, though has been used by others
(see &lt;a href=&quot;http://intro-to-dxr.cwyman.org/presentations/IntroDXR_ShaderTutorial.pdf&quot;&gt;Chris Wyman’s Intro to DXR&lt;/a&gt;
and &lt;a href=&quot;https://github.com/SaschaWillems/Vulkan/tree/master/data/shaders/nv_ray_tracing_shadows&quot;&gt;Sascha Willems’s NV Ray Tracing Shadows Example&lt;/a&gt;).
After switching to this approach in &lt;a href=&quot;https://github.com/Twinklebear/ChameleonRT&quot;&gt;ChameleonRT&lt;/a&gt;
I decided to run a small benchmark comparing some of the options for tracing shadow rays.
I’ll also discuss an extra trick we can use to simplify the shader binding table setup,
which lets us skip creating an occlusion hit group entirely.&lt;/p&gt;


				&lt;a href=&quot;https://www.willusher.io//graphics/2019/09/06/faster-shadow-rays-on-rtx&quot;&gt;Continue Reading&lt;/a&gt;
			</description>
			<link>https://www.willusher.io//graphics/2019/09/06/faster-shadow-rays-on-rtx</link>
			<guid>https://www.willusher.io//graphics/2019/09/06/faster-shadow-rays-on-rtx</guid>
			<pubDate>Fri, 06 Sep 2019 00:00:00 -0700</pubDate>
		</item>
		
		<item>
			<title>Volume Rendering with WebGL</title>
			<description>
&lt;figure&gt;
	&lt;img class=&quot;img-fluid&quot; src=&quot;https://i.imgur.com/YqdyKCj.png&quot; /&gt;
	
	&lt;figcaption&gt;&lt;i&gt;Figure 1:
	Example volume renderings, using the WebGL volume renderer described in this post.
	Left: A simulation of the spatial probability distribution
	of electrons in a high potential protein molecule.
	Right: A CT scan of a Bonsai Tree.
	Both datasets are from the
	&lt;a href=&quot;https://klacansky.com/open-scivis-datasets/&quot;&gt;Open SciVis Datasets&lt;/a&gt;
	repository.
	&lt;/i&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;In scientific visualization, volume rendering is widely used to visualize
3D scalar fields. These scalar fields are often
uniform grids of values, representing,
for example, charge density around a molecule,
an MRI or CT scan, air flow around an airplane, etc.
Volume rendering is a conceptually straightforward method
for turning such data into an image: by sampling the data
along rays from the eye and assigning
a color and transparency to each sample, we can
produce useful and beautiful images of such scalar fields
(see Figure 1).
In a GPU renderer, these 3D scalar fields are stored
as 3D textures; however, in WebGL1 3D textures were
not supported, requiring additional hacks to emulate them
for volume rendering.
Recently, WebGL2 added support for 3D textures,
allowing for an elegant and fast volume renderer to be
implemented entirely in the browser.
In this post we’ll discuss the mathematical background
for volume rendering, and how it can be implemented in
WebGL2 to create an interactive volume renderer
entirely in the browser!
Before we start, you can try out the volume renderer
described in this post &lt;a href=&quot;https://www.willusher.io/webgl-volume-raycaster/&quot;&gt;online&lt;/a&gt;.&lt;/p&gt;


				&lt;a href=&quot;https://www.willusher.io//webgl/2019/01/13/volume-rendering-with-webgl&quot;&gt;Continue Reading&lt;/a&gt;
			</description>
			<link>https://www.willusher.io//webgl/2019/01/13/volume-rendering-with-webgl</link>
			<guid>https://www.willusher.io//webgl/2019/01/13/volume-rendering-with-webgl</guid>
			<pubDate>Sun, 13 Jan 2019 00:00:00 -0800</pubDate>
		</item>
		
		<item>
			<title>Comments in LaTeX</title>
			<description>
&lt;p&gt;When writing a paper in LaTeX, it’s common to leave
notes and comments in the text, either to yourself
or your co-authors. I used to write these
as just different colored text using &lt;code class=&quot;highlighter-rouge&quot;&gt;\textcolor{...}&lt;/code&gt;,
with each author assigned a color, or all with the same color.
However, with more authors
it can get hard to keep picking legible font colors.
Futhermore, sometimes just a different color font doesn’t
stand out quite as much as I’d like from the rest of the text.
More recently I’ve switched to using highlights for
the comments, which works well with multiple authors,
and helps the comments stand out from the rest of
the text. This is easy to do with the
&lt;a href=&quot;https://ctan.org/pkg/soul?lang=en&quot;&gt;soul&lt;/a&gt; and
&lt;a href=&quot;https://ctan.org/pkg/xcolor?lang=en&quot;&gt;xcolor&lt;/a&gt; packages.&lt;/p&gt;


				&lt;a href=&quot;https://www.willusher.io//latex/2018/07/10/comments-in-latex&quot;&gt;Continue Reading&lt;/a&gt;
			</description>
			<link>https://www.willusher.io//latex/2018/07/10/comments-in-latex</link>
			<guid>https://www.willusher.io//latex/2018/07/10/comments-in-latex</guid>
			<pubDate>Tue, 10 Jul 2018 00:00:00 -0700</pubDate>
		</item>
		
		<item>
			<title>Distributed Rendering with Rust and Mio</title>
			<description>
&lt;p&gt;In this post we’ll take a look at adding distributed rendering to
&lt;a href=&quot;https://github.com/Twinklebear/tray_rust&quot;&gt;tray_rust&lt;/a&gt; which will let us take advantage of multiple
machines when rendering an image, like a compute cluster.
To do this we’ll look at options for how to distribute the rendering job across multiple nodes
and what sort of communication is needed synchronize their work. We’ll also look into how we
can use &lt;a href=&quot;https://github.com/carllerche/mio&quot;&gt;mio&lt;/a&gt; to write an efficient master process that
can manage multiple workers effectively.&lt;/p&gt;

&lt;p&gt;After implementing a simple technique to distribute the job we’ll discuss
the scalability of this approach and possible paths forward to improve it. I’ve
also recently written a &lt;a href=&quot;https://github.com/Twinklebear/tray_rust_blender&quot;&gt;plugin for Blender&lt;/a&gt; so you can
easily create your own scenes and
will mention a bit on how to run the ray tracer on Google Compute Engine (or AWS EC2)
if you want to try out the distributed rendering yourself.&lt;/p&gt;


				&lt;a href=&quot;https://www.willusher.io//2016/01/02/distributed-rendering-with-rust-and-mio&quot;&gt;Continue Reading&lt;/a&gt;
			</description>
			<link>https://www.willusher.io//2016/01/02/distributed-rendering-with-rust-and-mio</link>
			<guid>https://www.willusher.io//2016/01/02/distributed-rendering-with-rust-and-mio</guid>
			<pubDate>Sat, 02 Jan 2016 00:00:00 -0800</pubDate>
		</item>
		
		<item>
			<title>Rendering an Animation in Rust</title>
			<description>
&lt;p&gt;In this post we’ll look at adding a pretty awesome new feature to &lt;a href=&quot;https://github.com/Twinklebear/tray_rust&quot;&gt;tray_rust&lt;/a&gt;,
something I’ve never implemented before: animation! We’ll take a look at a simple way for sampling time in our scene, how
we can associate time points with transformations of objects to make them move and how to compute smooth animation
paths with B-Splines. Then we’ll wrap up with rendering a really cool animation by using 60 different
machines spread across two clusters at my lab.&lt;/p&gt;


				&lt;a href=&quot;https://www.willusher.io//2015/12/16/rendering-an-animation-in-rust&quot;&gt;Continue Reading&lt;/a&gt;
			</description>
			<link>https://www.willusher.io//2015/12/16/rendering-an-animation-in-rust</link>
			<guid>https://www.willusher.io//2015/12/16/rendering-an-animation-in-rust</guid>
			<pubDate>Wed, 16 Dec 2015 00:00:00 -0800</pubDate>
		</item>
		
		<item>
			<title>Porting a Ray Tracer to Rust, part 3</title>
			<description>
&lt;p&gt;It’s been a little while since my last post on tray_rust as I’ve been a busy with classes, but I’ve
had a bit of free time to implement some extremely cool features. In this post we’ll look at porting over
the path tracing code and adding a bounding volume hierarchy, along with adding support for triangle meshes and measured material data from the
&lt;a href=&quot;http://www.merl.com/brdf/&quot;&gt;MERL BRDF Database&lt;/a&gt; introduced by Matusik et al. in 2003 in
&lt;a href=&quot;http://dl.acm.org/citation.cfm?id=882343&quot;&gt;A Data-Driven Reflectance Model&lt;/a&gt;. In the process of implementing the BVH we’ll get a taste
of Rust’s generic programming facilities and use them to write a flexible BVH capable of storing any type that can
report its bounds. In the spirit of fogleman’s gorgeous &lt;a href=&quot;https://github.com/fogleman/pt&quot;&gt;Go Gopher in Go&lt;/a&gt; we’ll wrap up
by rendering the Rust logo in Rust using a model made by
&lt;a href=&quot;http://blenderartists.org/forum/showthread.php?362836-Rust-language-3D-logo&quot;&gt;Nylithius on BlenderArtists&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;If you’ve been following Rust’s development a bit you have probably noticed that the timing of this post is not a
coincidence, since Rust 1.0.0 is being released &lt;a href=&quot;http://blog.rust-lang.org/2015/05/15/Rust-1.0.html&quot;&gt;today&lt;/a&gt;!&lt;/p&gt;


				&lt;a href=&quot;https://www.willusher.io//2015/05/15/porting-a-ray-tracer-to-rust-part-3&quot;&gt;Continue Reading&lt;/a&gt;
			</description>
			<link>https://www.willusher.io//2015/05/15/porting-a-ray-tracer-to-rust-part-3</link>
			<guid>https://www.willusher.io//2015/05/15/porting-a-ray-tracer-to-rust-part-3</guid>
			<pubDate>Fri, 15 May 2015 00:00:00 -0700</pubDate>
		</item>
		
		<item>
			<title>Porting a Ray Tracer to Rust, part 2</title>
			<description>
&lt;p&gt;As mentioned in my &lt;a href=&quot;/2014/12/30/porting-a-ray-tracer-to-rust-part-1&quot;&gt;previous post&lt;/a&gt; I spent the past month-ish
working on improving both the rendering capabilities and performance of &lt;a href=&quot;https://github.com/Twinklebear/tray_rust&quot;&gt;tray_rust&lt;/a&gt;.
While it’s not yet capable of path tracing we can at least have light and shadow and shade our objects with diffuse or specularly
reflective and/or transmissive materials. Along with this I’ve improved performance by parallelizing
the rendering process using Rust’s multithreading capabilities. Although ray tracing is a trivially parallel task there are
two pieces of state that must be shared and modified between threads: the pixel/block counter and the framebuffer.
With Rust’s strong focus on safety I was worried that I would have to resort to unsafe blocks to share these
small pieces of mutable state but I found that the &lt;a href=&quot;http://doc.rust-lang.org/std/sync/index.html&quot;&gt;std::sync module&lt;/a&gt;
provided safe methods for everything I needed and performs quite well. While it’s difficult to compare against
&lt;a href=&quot;https://github.com/Twinklebear/tray&quot;&gt;tray&lt;/a&gt; (my initial C++ version) as the design of tray_rust has diverged quite 
a bit I’ll put some performance numbers in the multithreading section.&lt;/p&gt;

&lt;p&gt;During the past month &lt;a href=&quot;http://www.rust-lang.org/&quot;&gt;Rust&lt;/a&gt; has also seen some pretty large changes and is currently
in its 1.0 alpha release with the &lt;a href=&quot;http://blog.rust-lang.org/2014/12/12/1.0-Timeline.html&quot;&gt;first beta&lt;/a&gt;
fast approaching.&lt;/p&gt;


				&lt;a href=&quot;https://www.willusher.io//2015/01/30/porting-a-ray-tracer-to-rust-part-2&quot;&gt;Continue Reading&lt;/a&gt;
			</description>
			<link>https://www.willusher.io//2015/01/30/porting-a-ray-tracer-to-rust-part-2</link>
			<guid>https://www.willusher.io//2015/01/30/porting-a-ray-tracer-to-rust-part-2</guid>
			<pubDate>Fri, 30 Jan 2015 00:00:00 -0800</pubDate>
		</item>
		
		<item>
			<title>Porting a Ray Tracer to Rust, part 1</title>
			<description>
&lt;p&gt;I’ve decided to port over my physically based ray tracer &lt;a href=&quot;https://github.com/Twinklebear/tray&quot;&gt;tray&lt;/a&gt;
to &lt;a href=&quot;http://www.rust-lang.org/&quot;&gt;Rust&lt;/a&gt; to finally try out the language with a decent sized project.
In the series we’ll work through the implementation of a physically based ray tracer built
on the techniques discussed in &lt;a href=&quot;http://pbrt.org/&quot;&gt;Physically Based Rendering&lt;/a&gt;. I won’t go into a lot of
detail about rendering theory or the less exciting implementation details but will focus more on Rust
specific concerns and implementation decisions along with comparisons vs. my C++ version.
If you’re looking to learn more about ray tracing I highly recommend picking up Physically
Based Rendering and working through it. Hopefully throughout the series folks more experienced
with Rust can point out mistakes and improvements as well, since I have no experience with Rust
prior to this series.&lt;/p&gt;

&lt;p&gt;With the intro out of the way, let’s get started! Since it’s the beginning of
the series this is my first time really working with Rust and our goal is pretty simple: render a white
sphere and save the image.&lt;/p&gt;


				&lt;a href=&quot;https://www.willusher.io//2014/12/30/porting-a-ray-tracer-to-rust-part-1&quot;&gt;Continue Reading&lt;/a&gt;
			</description>
			<link>https://www.willusher.io//2014/12/30/porting-a-ray-tracer-to-rust-part-1</link>
			<guid>https://www.willusher.io//2014/12/30/porting-a-ray-tracer-to-rust-part-1</guid>
			<pubDate>Tue, 30 Dec 2014 00:00:00 -0800</pubDate>
		</item>
		
		<item>
			<title>Postscript 1: Easy Cleanup</title>
			<description>
&lt;p&gt;In this quick postscript we’ll look into a simple way to clean up our various SDL resources with variadic templates
and template specialization. This will let us clean up all our resources with a single simple call:
&lt;code class=&quot;highlighter-rouge&quot;&gt;cleanup(texA, texB, renderer, window)&lt;/code&gt; instead of calling all the corresponding &lt;code class=&quot;highlighter-rouge&quot;&gt;SDL_Destroy/Free*&lt;/code&gt; functions,
saving ourselves a lot of typing.&lt;/p&gt;

&lt;p&gt;We’ll do this by creating a variadic function &lt;code class=&quot;highlighter-rouge&quot;&gt;cleanup&lt;/code&gt; that will take the list of SDL resources to be free’d and then
define specializations of it for each resource we’ll be passing, eg. for &lt;code class=&quot;highlighter-rouge&quot;&gt;SDL_Window&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;SDL_Renderer&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;SDL_Texture&lt;/code&gt;
and so on.&lt;/p&gt;


				&lt;a href=&quot;https://www.willusher.io//sdl2%20tutorials/2014/08/01/postscript-1-easy-cleanup&quot;&gt;Continue Reading&lt;/a&gt;
			</description>
			<link>https://www.willusher.io//sdl2%20tutorials/2014/08/01/postscript-1-easy-cleanup</link>
			<guid>https://www.willusher.io//sdl2%20tutorials/2014/08/01/postscript-1-easy-cleanup</guid>
			<pubDate>Fri, 01 Aug 2014 00:00:00 -0700</pubDate>
		</item>
		
		<item>
			<title>Postscript 0: Properly Finding Resource Paths</title>
			<description>
&lt;p&gt;In this short postscript we’ll learn how to make use of &lt;a href=&quot;https://wiki.libsdl.org/SDL_GetBasePath&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;SDL_GetBasePath&lt;/code&gt;&lt;/a&gt; to
properly resolve the path to our resource directory where we’ll be storing all the assets needed for each lesson.
This approach lets us avoid issues with relative paths since it doesn’t depend on where the program working
directory is set when it’s run. This functionality was introduced in SDL 2.0.1 so if you haven’t updated to the latest SDL
be sure to grab that before getting started.&lt;/p&gt;


				&lt;a href=&quot;https://www.willusher.io//sdl2%20tutorials/2014/06/16/postscript-0-properly-finding-resource-paths&quot;&gt;Continue Reading&lt;/a&gt;
			</description>
			<link>https://www.willusher.io//sdl2%20tutorials/2014/06/16/postscript-0-properly-finding-resource-paths</link>
			<guid>https://www.willusher.io//sdl2%20tutorials/2014/06/16/postscript-0-properly-finding-resource-paths</guid>
			<pubDate>Mon, 16 Jun 2014 00:00:00 -0700</pubDate>
		</item>
		
		<item>
			<title>Lesson 0: CMake</title>
			<description>
&lt;p&gt;CMake is really useful for building the lessons since it lets us generate make files or project files for just about
any platform and IDE. It also helps with resolving dependencies (such as SDL2), platform specific configurations and
much much more. If you’re unfamiliar with CMake there’s a nice introduction
available on &lt;a href=&quot;http://www.cmake.org/cmake/help/cmake_tutorial.html&quot;&gt;their site&lt;/a&gt; to help you get started.&lt;/p&gt;


				&lt;a href=&quot;https://www.willusher.io//sdl2%20tutorials/2014/03/06/lesson-0-cmake&quot;&gt;Continue Reading&lt;/a&gt;
			</description>
			<link>https://www.willusher.io//sdl2%20tutorials/2014/03/06/lesson-0-cmake</link>
			<guid>https://www.willusher.io//sdl2%20tutorials/2014/03/06/lesson-0-cmake</guid>
			<pubDate>Thu, 06 Mar 2014 00:00:00 -0800</pubDate>
		</item>
		
		<item>
			<title>Lesson 6: True Type Fonts with SDL_ttf</title>
			<description>
&lt;p&gt;In this lesson we’ll see how to perform basic True Type font rendering with the SDL_ttf extension library.
Setting up the library is identical to what we did in
&lt;a href=&quot;/sdl2%20tutorials/2013/08/18/lesson-3-sdl-extension-libraries&quot;&gt;Lesson 3&lt;/a&gt; for SDL_image, but just replace
“image” with “ttf” (Windows users should also copy the included freetype dll over). So &lt;a href=&quot;http://www.libsdl.org/projects/SDL_ttf/&quot;&gt;download SDL_ttf&lt;/a&gt;,
take a peek at the &lt;a href=&quot;http://www.libsdl.org/projects/SDL_ttf/docs/index.html&quot;&gt;documentation&lt;/a&gt;, and let’s get started!&lt;/p&gt;


				&lt;a href=&quot;https://www.willusher.io//sdl2%20tutorials/2013/12/18/lesson-6-true-type-fonts-with-sdl_ttf&quot;&gt;Continue Reading&lt;/a&gt;
			</description>
			<link>https://www.willusher.io//sdl2%20tutorials/2013/12/18/lesson-6-true-type-fonts-with-sdl_ttf</link>
			<guid>https://www.willusher.io//sdl2%20tutorials/2013/12/18/lesson-6-true-type-fonts-with-sdl_ttf</guid>
			<pubDate>Wed, 18 Dec 2013 00:00:00 -0800</pubDate>
		</item>
		
		<item>
			<title>Lesson 5: Clipping Sprite Sheets</title>
			<description>
&lt;p&gt;It’s common in sprite based games to use a larger image file containing many smaller images, such as the 
tiles for a tileset, instead of having a separate image file for each tile. This type of image is known
as a sprite sheet and is very handy to work with since we don’t need to change which texture we’re drawing
each time but rather just which subsection of the texture.&lt;/p&gt;


				&lt;a href=&quot;https://www.willusher.io//sdl2%20tutorials/2013/08/27/lesson-5-clipping-sprite-sheets&quot;&gt;Continue Reading&lt;/a&gt;
			</description>
			<link>https://www.willusher.io//sdl2%20tutorials/2013/08/27/lesson-5-clipping-sprite-sheets</link>
			<guid>https://www.willusher.io//sdl2%20tutorials/2013/08/27/lesson-5-clipping-sprite-sheets</guid>
			<pubDate>Tue, 27 Aug 2013 00:00:00 -0700</pubDate>
		</item>
		
		<item>
			<title>Lesson 4: Handling Events</title>
			<description>
&lt;p&gt;In this lesson we’ll learn the basics of reading user input with SDL, in this simple example we’ll interpret any input
as the user wanting to quit our application.
To read events SDL provides the &lt;a href=&quot;http://wiki.libsdl.org/SDL_Event&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;SDL_Event&lt;/code&gt;&lt;/a&gt; structure
and functions to get events from the queue such as &lt;a href=&quot;http://wiki.libsdl.org/SDL_PollEvent&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;SDL_PollEvent&lt;/code&gt;&lt;/a&gt;.
The code for this lesson is built off of the lesson 3 code, if you need that code to start from grab it on &lt;a href=&quot;https://github.com/Twinklebear/TwinklebearDev-Lessons/tree/master/Lesson3&quot;&gt;Github&lt;/a&gt; and let’s get started!&lt;/p&gt;


				&lt;a href=&quot;https://www.willusher.io//sdl2%20tutorials/2013/08/20/lesson-4-handling-events&quot;&gt;Continue Reading&lt;/a&gt;
			</description>
			<link>https://www.willusher.io//sdl2%20tutorials/2013/08/20/lesson-4-handling-events</link>
			<guid>https://www.willusher.io//sdl2%20tutorials/2013/08/20/lesson-4-handling-events</guid>
			<pubDate>Tue, 20 Aug 2013 00:00:00 -0700</pubDate>
		</item>
		
		<item>
			<title>Lesson 3: SDL Extension Libraries</title>
			<description>
&lt;p&gt;Up until now we’ve only been using BMP images as they’re the only type supported by the base SDL library,
but being restricted to using BMP images isn’t that great. Fortunately there are a set of SDL extension libraries
that add useful features to SDL, such as support for a wide variety of image types through 
&lt;a href=&quot;http://www.libsdl.org/projects/SDL_image/&quot;&gt;SDL_image&lt;/a&gt;. The other available libraries are
&lt;a href=&quot;http://www.libsdl.org/projects/SDL_ttf/&quot;&gt;SDL_ttf&lt;/a&gt; which provides TTF rendering support, 
&lt;a href=&quot;http://www.libsdl.org/projects/SDL_net/&quot;&gt;SDL_net&lt;/a&gt; which provides low level networking
and &lt;a href=&quot;http://www.libsdl.org/projects/SDL_mixer/&quot;&gt;SDL_mixer&lt;/a&gt; which provides multi-channel audio playback.&lt;/p&gt;


				&lt;a href=&quot;https://www.willusher.io//sdl2%20tutorials/2013/08/18/lesson-3-sdl-extension-libraries&quot;&gt;Continue Reading&lt;/a&gt;
			</description>
			<link>https://www.willusher.io//sdl2%20tutorials/2013/08/18/lesson-3-sdl-extension-libraries</link>
			<guid>https://www.willusher.io//sdl2%20tutorials/2013/08/18/lesson-3-sdl-extension-libraries</guid>
			<pubDate>Sun, 18 Aug 2013 00:00:00 -0700</pubDate>
		</item>
		
		<item>
			<title>Lesson 2: Don&#39;t Put Everything in Main</title>
			<description>
&lt;p&gt;In this lesson we’ll begin organizing our texture loading and rendering code from the previous lesson by 
moving them out of main and placing them into some useful functions. We’ll also write a simple generic
SDL error logger and learn how images are positioned and scaled when rendering with SDL.&lt;/p&gt;


				&lt;a href=&quot;https://www.willusher.io//sdl2%20tutorials/2013/08/17/lesson-2-dont-put-everything-in-main&quot;&gt;Continue Reading&lt;/a&gt;
			</description>
			<link>https://www.willusher.io//sdl2%20tutorials/2013/08/17/lesson-2-dont-put-everything-in-main</link>
			<guid>https://www.willusher.io//sdl2%20tutorials/2013/08/17/lesson-2-dont-put-everything-in-main</guid>
			<pubDate>Sat, 17 Aug 2013 00:00:00 -0700</pubDate>
		</item>
		
		<item>
			<title>Lesson 1: Hello World</title>
			<description>
&lt;p&gt;In this lesson we’ll learn how to open a window, create a rendering context and draw
an image we’ve loaded to the screen. Grab the BMP we’ll be drawing below and save it somewhere in your
project and let’s get started!&lt;/p&gt;


				&lt;a href=&quot;https://www.willusher.io//sdl2%20tutorials/2013/08/17/lesson-1-hello-world&quot;&gt;Continue Reading&lt;/a&gt;
			</description>
			<link>https://www.willusher.io//sdl2%20tutorials/2013/08/17/lesson-1-hello-world</link>
			<guid>https://www.willusher.io//sdl2%20tutorials/2013/08/17/lesson-1-hello-world</guid>
			<pubDate>Sat, 17 Aug 2013 00:00:00 -0700</pubDate>
		</item>
		
		<item>
			<title>Lesson 0: Visual Studio</title>
			<description>
&lt;p&gt;Now that we’ve got the libraries installed we’ll want to create a new project to include and
link against SDL. At the end we’ll save this as a template project so in the future we can just
load our template and get to work. First we need a new empty C++ project.&lt;/p&gt;


				&lt;a href=&quot;https://www.willusher.io//sdl2%20tutorials/2013/08/15/lesson-0-visual-studio&quot;&gt;Continue Reading&lt;/a&gt;
			</description>
			<link>https://www.willusher.io//sdl2%20tutorials/2013/08/15/lesson-0-visual-studio</link>
			<guid>https://www.willusher.io//sdl2%20tutorials/2013/08/15/lesson-0-visual-studio</guid>
			<pubDate>Thu, 15 Aug 2013 00:00:00 -0700</pubDate>
		</item>
		
		<item>
			<title>Lesson 0: Setting Up SDL</title>
			<description>
&lt;p&gt;The first step is to get the SDL2 development libraries setup on your system, you can download
them from the &lt;a href=&quot;http://www.libsdl.org/download-2.0.php&quot;&gt;SDL2 downloads page&lt;/a&gt;.&lt;/p&gt;


				&lt;a href=&quot;https://www.willusher.io//sdl2%20tutorials/2013/08/15/lesson-0-setting-up-sdl&quot;&gt;Continue Reading&lt;/a&gt;
			</description>
			<link>https://www.willusher.io//sdl2%20tutorials/2013/08/15/lesson-0-setting-up-sdl</link>
			<guid>https://www.willusher.io//sdl2%20tutorials/2013/08/15/lesson-0-setting-up-sdl</guid>
			<pubDate>Thu, 15 Aug 2013 00:00:00 -0700</pubDate>
		</item>
		
		<item>
			<title>Lesson 0: MinGW</title>
			<description>
&lt;p&gt;To build the projects with mingw we’ll be using a lightweight makefile that will set the
include and library paths along with linking our dependencies for us. The makefile assumes
that you’ve placed the SDL mingw development libraries under &lt;code class=&quot;highlighter-rouge&quot;&gt;C:/SDL2-2.0.0-mingw/&lt;/code&gt; and that
you’re using the 32bit version of mingw and the 32bit libraries. You should change this to 
match your compiler (32/64bit) and the location of your SDL folder. To use makefiles with mingw call
&lt;code class=&quot;highlighter-rouge&quot;&gt;mingw32-make.exe&lt;/code&gt; in the folder containing the makefile.&lt;/p&gt;

&lt;p&gt;If you’re unfamiliar with Makefiles a basic introduction can be found &lt;a href=&quot;http://mrbook.org/blog/tutorials/make/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;


				&lt;a href=&quot;https://www.willusher.io//sdl2%20tutorials/2013/08/15/lesson-0-mingw&quot;&gt;Continue Reading&lt;/a&gt;
			</description>
			<link>https://www.willusher.io//sdl2%20tutorials/2013/08/15/lesson-0-mingw</link>
			<guid>https://www.willusher.io//sdl2%20tutorials/2013/08/15/lesson-0-mingw</guid>
			<pubDate>Thu, 15 Aug 2013 00:00:00 -0700</pubDate>
		</item>
		
		<item>
			<title>Lesson 0: Mac Command Line</title>
			<description>
&lt;p&gt;To build the projects on OS X we’ll be using a simple makefile that will include the framework for us.
The makefile assumes you’ve installed SDL following the instructions in the .dmg file on the SDL2
downloads page and now have it available as a framework.&lt;/p&gt;

&lt;p&gt;If you’re unfamiliar with Makefiles a basic introduction can be found &lt;a href=&quot;http://mrbook.org/blog/tutorials/make/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;


				&lt;a href=&quot;https://www.willusher.io//sdl2%20tutorials/2013/08/15/lesson-0-mac-command-line&quot;&gt;Continue Reading&lt;/a&gt;
			</description>
			<link>https://www.willusher.io//sdl2%20tutorials/2013/08/15/lesson-0-mac-command-line</link>
			<guid>https://www.willusher.io//sdl2%20tutorials/2013/08/15/lesson-0-mac-command-line</guid>
			<pubDate>Thu, 15 Aug 2013 00:00:00 -0700</pubDate>
		</item>
		
		<item>
			<title>Lesson 0: Linux Command Line</title>
			<description>
&lt;p&gt;To build the projects on Linux we’ll be using a simple makefile that will setup the include and library
dependencies for us. The makefile assumes that your SDL libraries are installed under &lt;code class=&quot;highlighter-rouge&quot;&gt;/usr/local/lib&lt;/code&gt;
and the headers are under &lt;code class=&quot;highlighter-rouge&quot;&gt;/usr/local/include&lt;/code&gt;. These are the install locations if you built the
project through cmake, some more detail on building from source can be found &lt;a href=&quot;http://twinklebear.github.io/sdl2%20tutorials/2013/08/15/lesson-0-linux-command-line/#comment-1053605032&quot;&gt;here&lt;/a&gt;. 
If you’ve installed it through your package manager or placed the libraries 
and headers elsewhere you may need to change these paths to match your installation. You can also check the output
of &lt;code class=&quot;highlighter-rouge&quot;&gt;sdl2-config&lt;/code&gt; with the &lt;code class=&quot;highlighter-rouge&quot;&gt;--cflags&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;--libs&lt;/code&gt; switches to locate your install, assuming you haven’t moved it.&lt;/p&gt;

&lt;p&gt;If you’re unfamiliar with Makefiles a basic introduction can be found &lt;a href=&quot;http://mrbook.org/blog/tutorials/make/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;


				&lt;a href=&quot;https://www.willusher.io//sdl2%20tutorials/2013/08/15/lesson-0-linux-command-line&quot;&gt;Continue Reading&lt;/a&gt;
			</description>
			<link>https://www.willusher.io//sdl2%20tutorials/2013/08/15/lesson-0-linux-command-line</link>
			<guid>https://www.willusher.io//sdl2%20tutorials/2013/08/15/lesson-0-linux-command-line</guid>
			<pubDate>Thu, 15 Aug 2013 00:00:00 -0700</pubDate>
		</item>
		
		
		<item>
			<title>Scalable Ray Tracing Using the Distributed FrameBuffer</title>
			<description>
				
				&lt;figure&gt;
					&lt;img src=&quot;https://i.imgur.com/xDw1wI1.jpg&quot;/&gt;
					&lt;figcaption&gt;
					&lt;b&gt;Fig 1:&lt;/b&gt;&lt;i&gt;Large-scale interactive visualization using the Distributed FrameBuffer. Top left: Image-parallel rendering of two transparent isosurfaces from the Richtmyer-Meshkov (516M triangles), 8FPS with a 2048&lt;sup&gt;2&lt;/sup&gt; framebuffer using 16 Stampede2 Intel Xeon Platinum 8160 SKX nodes. Top right: Data-parallel rendering of the Cosmic Web (29B transparent spheres), 2FPS at 2048&lt;sup&gt;2&lt;/sup&gt; using 128 Theta Intel Xeon Phi Knight&#39;s Landing (KNL) nodes. Bottom: Data-parallel rendering of the 951GB DNS volume combined with a transparent isosurface (4.35B triangles), 5FPS at 4096x1024 using 64 Stampede2 Intel Xeon Phi KNL nodes.&lt;/i&gt;
					&lt;/figcaption&gt;
				&lt;/figure&gt;
				
				Image- and data-parallel rendering across multiple nodes on high-performance computing systems is widely used in visualization to provide higher frame rates, support large data sets, and render data in situ. Specifically for in situ visualization, reducing bottlenecks incurred by the visualization and compositing is of key concern to reduce the overall simulation runtime. Moreover, prior algorithms have been designed to support either image- or data-parallel rendering and impose restrictions on the data distribution, requiring different implementations for each configuration. In this paper, we introduce the Distributed FrameBuffer, an asynchronous image-processing framework for multi-node rendering. We demonstrate that our approach achieves performance superior to the state of the art for common use cases, while providing the flexibility to support a wide range of parallel rendering algorithms and data distributions. By building on this framework, we extend the open-source ray tracing library OSPRay with a data-distributed API, enabling its use in data-distributed and in situ visualization applications.
				&lt;a href=&quot;https://www.willusher.io//publications/dfb&quot;&gt;Continue Reading&lt;/a&gt;
			</description>
			<link>https://www.willusher.io/publications/dfb</link>
			<guid>https://www.willusher.io/publications/dfb</guid>
			<pubDate>Tue, 09 Jul 2019 23:00:00 -0700</pubDate>
		</item>
		
		<item>
			<title>Ray Tracing Generalized Tube Primitives: Method and Applications</title>
			<description>
				
				&lt;figure&gt;
					&lt;img src=&quot;https://i.imgur.com/tTv2l2j.jpg&quot;/&gt;
					&lt;figcaption&gt;
					&lt;b&gt;Fig 1:&lt;/b&gt;&lt;i&gt;Visualizations using our &quot;generalized tube&quot; primitives. (a): DTI tractography data, semi-transparent fixed-radius streamlines (218K line segments). (b): A generated neuron assembly test case, streamlines with varying radii and bifurcations (3.2M l. s.). (c): Aneurysm morphology, semi-transparent streamlines with varying radii and bifurcations (3.9K l. s.) and an opaque center line with fixed radius and bifurcations (3.9K l. s.). (d): A tornado simulation, with radius used to encode the velocity magnitude (3.56M l. s.). (e): Flow past a torus, fixed-radius pathlines (6.5M l. s.). Rendered at: (a) 0.38FPS, (b) 7.2FPS, (c) 0.25FPS, (d) 18.8FPS, with a 2048x2048 framebuffer; (e) 23FPS with a 2048x786 framebuffer. Performance measured on a dual Intel Xeon E5-2640 v4 workstation, with shadows and ambient occlusion.&lt;/i&gt;
					&lt;/figcaption&gt;
				&lt;/figure&gt;
				
				We present a general high-performance technique for ray tracing generalized tube primitives. Our technique efficiently supports tube primitives with fixed and varying radii, general acyclic graph structures with bifurcations, and correct transparency with interior surface removal. Such tube primitives are widely used in scientific visualization to represent diffusion tensor imaging tractographies, neuron morphologies, and scalar or vector fields of 3D flow. We implement our approach within the OSPRay ray tracing framework, and evaluate it on a range of interactive visualization use cases of fixed- and varying-radius streamlines, pathlines, complex neuron morphologies, and brain tractographies. Our proposed approach provides interactive, high-quality rendering, with low memory overhead.
				&lt;a href=&quot;https://www.willusher.io//publications/tubes&quot;&gt;Continue Reading&lt;/a&gt;
			</description>
			<link>https://www.willusher.io/publications/tubes</link>
			<guid>https://www.willusher.io/publications/tubes</guid>
			<pubDate>Tue, 09 Jul 2019 23:00:00 -0700</pubDate>
		</item>
		
		<item>
			<title>Efficient Space Skipping and Adaptive Sampling of Unstructured Volumes Using Hardware Accelerated Ray Tracing</title>
			<description>
				
				&lt;figure&gt;
					&lt;img src=&quot;https://i.imgur.com/NgoH3iw.jpg&quot;/&gt;
					&lt;figcaption&gt;
					&lt;b&gt;Fig 1:&lt;/b&gt;&lt;i&gt;Performance improvement of our method on the 278 million tetrahedra Japan Earthquake data set. (a) A reference volume ray marcher without our method, at 0.9 FPS (1024&lt;sup&gt;2&lt;/sup&gt; pixels) on an NVIDIA RTX 8000 GPU. (b) A heat map of relative cost per-pixel in (a). (c) and (d), the same, but now with our space skipping and adaptive sampling method, running at 7 FPS (7× faster).&lt;/i&gt;
					&lt;/figcaption&gt;
				&lt;/figure&gt;
				
				Sample based ray marching is an effective method for direct volume rendering of unstructured meshes. However, sampling such meshes remains expensive, and strategies to reduce the number of samples taken have received relatively little attention. In this paper, we introduce a method for rendering unstructured meshes using a combination of a coarse spatial acceleration structure and hardware-accelerated ray tracing. Our approach enables efficient empty space skipping and adaptive sampling of unstructured meshes, and outperforms a reference ray marcher by up to 7×
				&lt;a href=&quot;https://www.willusher.io//publications/rtx-space-skipping&quot;&gt;Continue Reading&lt;/a&gt;
			</description>
			<link>https://www.willusher.io/publications/rtx-space-skipping</link>
			<guid>https://www.willusher.io/publications/rtx-space-skipping</guid>
			<pubDate>Sat, 19 Oct 2019 23:00:00 -0700</pubDate>
		</item>
		
		<item>
			<title>RTX Beyond Ray Tracing: Exploring the Use of Hardware Ray Tracing Cores for Tet-Mesh Point Location</title>
			<description>
				
				&lt;figure&gt;
					&lt;img src=&quot;https://i.imgur.com/clMvtsl.png&quot;/&gt;
					&lt;figcaption&gt;
					&lt;b&gt;Fig 1:&lt;/b&gt;&lt;i&gt;a-c) Illustrations of the tetrahedral mesh point location kernels evaluated in this paper. a) Our reference method builds a BVH over the tets and performs both BVH traversal and point-in-tet tests in software (black) using CUDA. b) &lt;font face=&#39;monospace&#39;&gt;rtx-bvh&lt;/font&gt; uses an RTX-accelerated BVH over tets and triggers hardware BVH traversal (green) by tracing infinitesimal rays at the sample points, while still performing point-tet tests in software (black). c) &lt;font face=&#39;monospace&#39;&gt;rtx-rep-faces&lt;/font&gt; and &lt;font face=&#39;monospace&#39;&gt;rtx-shrd-faces&lt;/font&gt; use both hardware BVH traversal and triangle intersection (green) by tracing rays against the tetrahedras&#39; faces. d) An image from the unstructured-data volume ray marcher used to evaluate our point location kernels, showing the 35.7M tet Agulhas Current data set rendered interactively on an NVIDIA TITAN RTX (34FPS at 1024&lt;sup&gt;2&lt;/sup&gt; pixels)&lt;/i&gt;
					&lt;/figcaption&gt;
				&lt;/figure&gt;
				
				We explore a first proof-of-concept example of creatively using the Turing generation&#39;s hardware ray tracing cores to solve a problem other than classical ray tracing, specifically, point location in unstructured tetrahedral meshes. Starting with a CUDA reference method, we describe and evaluate three different approaches to reformulate this problem in a manner that allows it to be mapped to these new hardware units. Each variant replaces the simpler problem of point queries with the more complex one of ray queries; however, thanks to hardware acceleration, these approaches are actually faster than the reference method.
				&lt;a href=&quot;https://www.willusher.io//publications/rtx-points&quot;&gt;Continue Reading&lt;/a&gt;
			</description>
			<link>https://www.willusher.io/publications/rtx-points</link>
			<guid>https://www.willusher.io/publications/rtx-points</guid>
			<pubDate>Sun, 07 Jul 2019 23:00:00 -0700</pubDate>
		</item>
		
		<item>
			<title>Spatially-aware Parallel I/O for Particle Data</title>
			<description>
				
				&lt;figure&gt;
					&lt;img src=&quot;/assets/img/icpp-two-phase-io.svg&quot;/&gt;
					&lt;figcaption&gt;
					&lt;b&gt;Fig 1:&lt;/b&gt;&lt;i&gt;An illustration of our two-phase I/O approach, which takes spatial locality into consideration.&lt;/i&gt;
					&lt;/figcaption&gt;
				&lt;/figure&gt;
				
				Particle data are used across a diverse set of large scale simulations, for example, in cosmology, molecular dynamics and combustion. At scale these applications generate tremendous amounts of data, which is often saved in an unstructured format that does not preserve spatial locality; resulting in poor read performance for post-processing analysis and visualization tasks, which typically make spatial queries. In this work, we explore some of the challenges of large scale particle data management, and introduce new techniques to perform scalable, spatially-aware write and read operations. We propose an adaptive aggregation technique to improve the performance of data aggregation, for both uniform and non-uniform particle distributions. Furthermore, we enable efficient read operations by employing a level of detail re-ordering and a multi-resolution layout. Finally, we demonstrate the scalability of our techniques with experiments on large scale simulation workloads up to 256K cores on two different leadership supercomputers, Mira and Theta.
				&lt;a href=&quot;https://www.willusher.io//publications/icpp19&quot;&gt;Continue Reading&lt;/a&gt;
			</description>
			<link>https://www.willusher.io/publications/icpp19</link>
			<guid>https://www.willusher.io/publications/icpp19</guid>
			<pubDate>Sun, 04 Aug 2019 23:00:00 -0700</pubDate>
		</item>
		
		<item>
			<title>CPU Isosurface Ray Tracing of Adaptive Mesh Refinement Data</title>
			<description>
				
				&lt;figure&gt;
					&lt;img src=&quot;https://i.imgur.com/PqmRTuz.jpg&quot;/&gt;
					&lt;figcaption&gt;
					&lt;b&gt;Fig 1:&lt;/b&gt;&lt;i&gt;High-fidelity isosurface visualizations of gigascale block-structured adaptive mesh refinement (BS-AMR) data using our method. Left: a 28GB GR-Chombo simulation of gravitational waves resulting from the collision of two black holes. Middle and Right: a 57GB AMR dataset computed with LAVA at NASA, simulating multiple fields over the landing gear of an aircraft. Middle: isosurface representation of the vorticity, rendered with path tracing. Right: a combined visualization of volume rending and an isosurface of the pressure over the landing gear, rendered with OSPRay&#39;s SciVis renderer.  Using our approach for ray tracing such AMR data, we can interactively render crack-free implicit isosurfaces in combination with direct volume rendering and advanced shading effects like transparency, ambient occlusion and path tracing.&lt;/i&gt;
					&lt;/figcaption&gt;
				&lt;/figure&gt;
				
				Adaptive mesh refinement (AMR) is a key technology for large-scale simulations that allows for adaptively changing the simulation mesh resolution, resulting in significant computational and storage savings. However, visualizing such AMR data poses a significant challenge due to the difficulties introduced by the hierarchical representation when reconstructing continuous field values. In this paper, we detail a comprehensive solution for interactive isosurface rendering of block-structured AMR data. We contribute a novel reconstruction strategy&amp;mdash;the &lt;i&gt;octant&lt;/i&gt; method&amp;mdash;which is continuous, adaptive and simple to implement. Furthermore, we present a generally applicable hybrid implicit isosurface ray-tracing method, which provides better rendering quality and performance than the built-in sampling-based approach in OSPRay. Finally, we integrate our &lt;i&gt;octant&lt;/i&gt; method and hybrid isosurface geometry into OSPRay as a module, providing the ability to create high-quality interactive visualizations combining volume and isosurface representations of BS-AMR data. We evaluate the rendering performance, memory consumption and quality of our method on two gigascale block-structured AMR datasets.
				&lt;a href=&quot;https://www.willusher.io//publications/amr-iso&quot;&gt;Continue Reading&lt;/a&gt;
			</description>
			<link>https://www.willusher.io/publications/amr-iso</link>
			<guid>https://www.willusher.io/publications/amr-iso</guid>
			<pubDate>Mon, 31 Dec 2018 22:00:00 -0800</pubDate>
		</item>
		
		<item>
			<title>A Virtual Reality Visualization Tool for Neuron Tracing</title>
			<description>
				
				&lt;figure&gt;
					&lt;img src=&quot;https://i.imgur.com/2rmTeh2.png&quot;/&gt;
					&lt;figcaption&gt;
					&lt;b&gt;Fig 1:&lt;/b&gt;&lt;i&gt;A screenshot of our VR neuron tracing tool using the isosurface rendering mode. The dark gray floor represents the extent of the tracked space. Users can orient themselves in the dataset via the minimap (right), which shows the world extent in blue, the current focus region in orange, and the previously traced neuronal structures. The focus region is displayed in the center of the space. The 3D interaction and visualization provides an intuitive environment for exploring the data and a natural interface for neuron tracing, resulting in faster, high-quality traces with less fatigue reported by users compared to existing 2D tools.&lt;/i&gt;
					&lt;/figcaption&gt;
				&lt;/figure&gt;
				
				Tracing neurons in large-scale microscopy data is crucial to establishing a wiring diagram of the brain, which is needed to understand how neural circuits in the brain process information and generate behavior. Automatic techniques often fail for large and complex datasets, and connectomics researchers may spend weeks or months manually tracing neurons using 2D image stacks. We present a design study of a new virtual reality (VR) system, developed in collaboration with trained neuroanatomists, to trace neurons in microscope scans of the visual cortex of primates. We hypothesize that using consumer-grade VR technology to interact with neurons directly in 3D will help neuroscientists better resolve complex cases and enable them to trace neurons faster and with less physical and mental strain. We discuss both the design process and technical challenges in developing an interactive system to navigate and manipulate terabyte-sized image volumes in VR. Using a number of different datasets, we demonstrate that, compared to widely used commercial software, consumer-grade VR presents a promising alternative for scientists.
				&lt;a href=&quot;https://www.willusher.io//publications/vrnt&quot;&gt;Continue Reading&lt;/a&gt;
			</description>
			<link>https://www.willusher.io/publications/vrnt</link>
			<guid>https://www.willusher.io/publications/vrnt</guid>
			<pubDate>Sun, 31 Dec 2017 22:00:00 -0800</pubDate>
		</item>
		
		<item>
			<title>libIS: A Lightweight Library for Flexible In Transit Visualization</title>
			<description>
				
				&lt;figure&gt;
					&lt;img src=&quot;https://i.imgur.com/UYlTqhT.png&quot;/&gt;
					&lt;figcaption&gt;
					&lt;b&gt;Fig 1:&lt;/b&gt;&lt;i&gt;Interactive in situ visualization of a 172k atom simulation of silicene formation with 128 LAMMPS ranks sending to 16 OSPRay renderer ranks, all executed on Theta in the mpi-multi configuration. When taking four ambient occlusion samples per-pixel, our viewer averages 7FPS at 1024x1024. Simulation dataset is courtesy of &lt;a href=&#39;https://pubs.rsc.org/en/content/articlelanding/2017/nr/c7nr03153j#!divAbstract&#39;&gt;Cherukara et al.&lt;/a&gt;&lt;/i&gt;
					&lt;/figcaption&gt;
				&lt;/figure&gt;
				
				As simulations grow in scale, the need for in situ analysis methods to handle the large data produced grows correspondingly. One desirable approach to in situ visualization is in transit visualization. By decoupling the simulation and visualization code, in transit approaches alleviate common difficulties with regard to the scalability of the analysis, ease of integration, usability, and impact on the simulation. We present libIS, a lightweight, flexible library which lowers the bar for using in transit visualization. Our library works on the concept of abstract regions of space containing data, which are transferred from the simulation to the visualization clients upon request, using a client-server model. We also provide a SENSEI analysis adaptor, which allows for transparent deployment of in transit visualization. We demonstrate the flexibility of our approach on batch analysis and interactive visualization use cases on different HPC resources.
				&lt;a href=&quot;https://www.willusher.io//publications/libis-isav18&quot;&gt;Continue Reading&lt;/a&gt;
			</description>
			<link>https://www.willusher.io/publications/libis-isav18</link>
			<guid>https://www.willusher.io/publications/libis-isav18</guid>
			<pubDate>Sun, 11 Nov 2018 22:00:00 -0800</pubDate>
		</item>
		
		<item>
			<title>VisIt-OSPRay: Toward an Exascale Volume Visualization System</title>
			<description>
				
				&lt;figure&gt;
					&lt;img src=&quot;https://i.imgur.com/JgAOmst.png&quot;/&gt;
					&lt;figcaption&gt;
					&lt;b&gt;Fig 1:&lt;/b&gt;&lt;i&gt;High-quality interactive volume visualization using VisIt-OSPRay: &lt;b&gt;a)&lt;/b&gt; volume rendering of O&lt;sub&gt;2&lt;/sub&gt; concentration inside a combustion chamber, data courtesy of the &lt;a href=&quot;http://ccmsc.sci.utah.edu/&quot;&gt;University of Utah CCMSC&lt;/a&gt;; &lt;b&gt;b)&lt;/b&gt; volume rendering of the Richtmyer-Meshkov Instability; &lt;b&gt;c)&lt;/b&gt; visualization of a supernova simulation; &lt;b&gt;d)&lt;/b&gt; visualization of the aneurysm dataset using volume rendering and streamlines; &lt;b&gt;e)&lt;/b&gt; scalable volume rendering of the 966GB DNS data on 64 Stampede2 Intel Xeon Phi Knight&#39;s Landing nodes.&lt;/i&gt;
					&lt;/figcaption&gt;
				&lt;/figure&gt;
				
				Large-scale simulations can easily produce data in excess of what can be efficiently visualized using production visualization software, making it challenging for scientists to gain insights from the results of these simulations. This trend is expected to grow with exascale. To meet this challenge, and run on the highly parallel hardware being deployed on HPC system, rendering systems in production visualization software must be redesigned to perform well at these new scales and levels of parallelism. In this work, we present VisIt-OSPRay, a high-performance, scalable, hybrid-parallel rendering system in VisIt, using OSPRay and IceT, coupled with PIDX for scalable I/O. We examine the scalability and memory efficiency of this system and investigate further areas for improvement to prepare VisIt for upcoming exascale workloads.
				&lt;a href=&quot;https://www.willusher.io//publications/visit-ospray&quot;&gt;Continue Reading&lt;/a&gt;
			</description>
			<link>https://www.willusher.io/publications/visit-ospray</link>
			<guid>https://www.willusher.io/publications/visit-ospray</guid>
			<pubDate>Sun, 03 Jun 2018 23:00:00 -0700</pubDate>
		</item>
		
		<item>
			<title>Scalable Data Management of the Uintah Simulation Framework for Next-Generation Engineering Problems with Radiation</title>
			<description>
				
				The need to scale next-generation industrial engineering problems to the largest computational platforms presents unique challenges. This paper focuses on data management related problems faced by the Uintah simulation framework at a production scale of 260K processes. Uintah provides a highly scalable asynchronous many-task runtime system, which in this work is used for the modeling of a 1000 megawatt electric (MWe) ultra-supercritical (USC) coal boiler. At 260K processes, we faced both parallel I/O and visualization related challenges, e.g., the default file-per-process I/O approach of Uintah did not scale on Mira. In this paper we present a simple to implement, restructuring based parallel I/O technique. We impose a restructuring step that alters the distribution of data among processes. The goal is to distribute the dataset such that each process holds a larger chunk of data, which is then written to a file independently. This approach finds a middle ground between two of the most common parallel I/O schemes–file per process I/O and shared file I/O–in terms of both the total number of generated files, and the extent of communication involved during the data aggregation phase. To address scalability issues when visualizing the simulation data, we developed a lightweight renderer using OSPRay, which allows scientists to visualize the data interactively at high quality and make production movies. Finally, this work presents a highly efficient and scalable radiation model based on the sweeping method, which significantly outperforms previous approaches in Uintah, like discrete ordinates. The integrated approach allowed the USC boiler problem to run on 260K CPU cores on Mira.
				&lt;a href=&quot;https://www.willusher.io//publications/scasia18&quot;&gt;Continue Reading&lt;/a&gt;
			</description>
			<link>https://www.willusher.io/publications/scasia18</link>
			<guid>https://www.willusher.io/publications/scasia18</guid>
			<pubDate>Mon, 19 Mar 2018 23:00:00 -0700</pubDate>
		</item>
		
		<item>
			<title>CPU Volume Rendering of Adaptive Mesh Refinement Data</title>
			<description>
				
				&lt;figure&gt;
					&lt;img src=&quot;https://i.imgur.com/CqZc3VJ.png&quot;/&gt;
					&lt;figcaption&gt;
					&lt;b&gt;Fig 1:&lt;/b&gt;&lt;i&gt;Two examples of our method (integrated within the OSPRay ray tracer): Left: 1.8GB Cosmos AMR data, rendered in ParaView. Right: a 57GB NASA Chombo simulation, rendered with ambient occlusion and shadows alongside mesh geometry.&lt;/i&gt;
					&lt;/figcaption&gt;
				&lt;/figure&gt;
				
				Adaptive Mesh Refinement (AMR) methods are widespread in scientific computing, and visualizing the resulting data with efficient and accurate rendering methods can be vital for enabling interactive data exploration. In this work, we detail a comprehensive solution for directly volume rendering block-structured (Berger-Colella) AMR data in the OSPRay interactive CPU ray tracing framework. In particular, we contribute a general method for representing and traversing AMR data using a kd-tree structure, and four different reconstruction options, one of which in particular (the basis function approach) is novel compared to existing methods. We demonstrate our system on two types of block-structured AMR data and compressed scalar field data, and show how it can be easily used in existing production-ready applications through a prototypical integration in the widely used visualization program ParaView.
				&lt;a href=&quot;https://www.willusher.io//publications/cvamr&quot;&gt;Continue Reading&lt;/a&gt;
			</description>
			<link>https://www.willusher.io/publications/cvamr</link>
			<guid>https://www.willusher.io/publications/cvamr</guid>
			<pubDate>Sun, 26 Nov 2017 22:00:00 -0800</pubDate>
		</item>
		
		<item>
			<title>Progressive CPU Volume Rendering with Sample Accumulation</title>
			<description>
				
				&lt;figure&gt;
					&lt;img src=&quot;https://i.imgur.com/15y1f8I.png&quot;/&gt;
					&lt;figcaption&gt;
					&lt;b&gt;Fig 1:&lt;/b&gt;&lt;i&gt;(a-c) Progressive refinement with Sample-Accumulation Volume Rendering (SAVR) on the 40GB Landing Gear AMR dataset using a prototype AMR sampler. The SAVR algorithm correctly accumulates frames to progressively refine the image. After 16 frames of accumulation the volume is sampled at the Nyquist limit, with some small noise, by 32 frames the noise has been removed. SAVR extends to distributed data, in (d) we show the 1TB DNS dataset, a 10240×7680×1536 uniform grid, rendered interactively across 64 second-generation Intel Xeon Phi &quot;Knights Landing&quot; (KNL) processor nodes on Stampede 1.5 at a 6144×1024 resolution. While interacting, our method achieves around 5.73 FPS.&lt;/i&gt;
					&lt;/figcaption&gt;
				&lt;/figure&gt;
				
				We present a new method for progressive volume rendering by accumulating object-space samples over successively rendered frames. Existing methods for progressive refinement either use image space methods or average pixels over frames, which can blur features or integrate incorrectly with respect to depth. Our approach stores samples along each ray, accumulates new samples each frame into a buffer, and progressively interleaves and integrates these samples. Though this process requires additional memory, it ensures interactivity and is well suited for CPU architectures with large memory and cache. This approach also extends well to distributed rendering in cluster environments. We implement this technique in Intel’s open source OSPRay CPU ray tracing framework and demonstrate that it is particularly useful for rendering volumetric data with costly sampling functions.
				&lt;a href=&quot;https://www.willusher.io//publications/savr&quot;&gt;Continue Reading&lt;/a&gt;
			</description>
			<link>https://www.willusher.io/publications/savr</link>
			<guid>https://www.willusher.io/publications/savr</guid>
			<pubDate>Sun, 11 Jun 2017 23:00:00 -0700</pubDate>
		</item>
		
		<item>
			<title>In Situ Exploration of Particle Simulations with CPU Ray Tracing</title>
			<description>
				
				&lt;figure&gt;
					&lt;img src=&quot;https://i.imgur.com/DO3JqOb.png&quot;/&gt;
					&lt;figcaption&gt;
					&lt;b&gt;Fig 1:&lt;/b&gt;&lt;i&gt;A coal particle combustion simulation in Uintah at three different timesteps with (left to right): 34.61M, 48.46M and 55.39M particles, with attribute based culling showing the full jet (top) and the front in detail (bottom). Using our in situ library to query and send data to our rendering client in OSPRay these images are rendered interactively with ambient occlusion, averaging around 13 FPS at 1920×1080. The renderer is run on 12 nodes of the Stampede supercomputer and pulls data from a Uintah simulation running on 64 processes (4 nodes). Our loosely-coupled in situ approach allows for live exploration at the full temporal fidelity of the simulation, without prohibitive IO cost.&lt;/i&gt;
					&lt;/figcaption&gt;
				&lt;/figure&gt;
				
				We present a system for interactive in situ visualization of large particle simulations, suitable for general CPU-based HPC architectures. As simulations grow in scale, in situ methods are needed to alleviate IO bottlenecks and visualize data at full spatio-temporal resolution. We use a lightweight loosely-coupled layer serving distributed data from the simulation to a data-parallel renderer running in separate processes. Leveraging the OSPRay ray tracing framework for visualization and balanced P-k-d trees, we can render simulation data in real-time, as they arrive, with negligible memory overhead. This flexible solution allows users to perform exploratory in situ visualization on the same computational resources as the simulation code, on dedicated visualization clusters or remote workstations, via a standalone rendering client that can be connected or disconnected as needed. We evaluate this system on simulations with up to 227M particles in the LAMMPS and Uintah computational frameworks, and show that our approach provides many of the advantages of tightly-coupled systems, with the flexibility to render on a wide variety of remote and co-processing resources.
				&lt;a href=&quot;https://www.willusher.io//publications/isp-jsfi&quot;&gt;Continue Reading&lt;/a&gt;
			</description>
			<link>https://www.willusher.io/publications/isp-jsfi</link>
			<guid>https://www.willusher.io/publications/isp-jsfi</guid>
			<pubDate>Fri, 30 Sep 2016 23:00:00 -0700</pubDate>
		</item>
		
		<item>
			<title>VTK-m: Accelerating the Visualization Toolkit for Massively Threaded Architectures</title>
			<description>
				
				One of the most critical challenges for high-performance computing (HPC) scientific visualization is execution on massively threaded processors. Of the many fundamental changes we are seeing in HPC systems, one of the most profound is a reliance on new processor types optimized for execution bandwidth over latency hiding. Our current production scientific visualization software is not designed for these new types of architectures. To address this issue, the VTK-m framework serves as a container for algorithms, provides flexible data representation, and simplifies the design of visualization algorithms on new and future computer architecture.
				&lt;a href=&quot;https://www.willusher.io//publications/vtkm&quot;&gt;Continue Reading&lt;/a&gt;
			</description>
			<link>https://www.willusher.io/publications/vtkm</link>
			<guid>https://www.willusher.io/publications/vtkm</guid>
			<pubDate>Sun, 08 May 2016 23:00:00 -0700</pubDate>
		</item>
		
		<item>
			<title>CPU Ray Tracing Large Particle Data with Balanced P-k-d Trees</title>
			<description>
				
				&lt;figure&gt;
					&lt;img src=&quot;https://i.imgur.com/1YNpRJ1.png&quot;/&gt;
					&lt;figcaption&gt;
					&lt;b&gt;Fig 1:&lt;/b&gt;&lt;i&gt;Full-detail ray tracing of giga-particle data sets. From left to right: CosmicWeb early universe data set from a P3D simulation with 29 billion particles; a 100 million atom molecular dynamics Al&lt;sub&gt;2&lt;/sub&gt;O&lt;sub&gt;3&lt;/sub&gt;−SiC materials fracture simulation; and a 1.3 billion particle Uintah MPM detonation simulation. Using a quad-socket, 72-core 2.5GHz Intel Xeon E7-8890 v3 Processor with 3TB RAM and path-tracing with progressive refinement at 1 sample per pixel, these far and close images (above and below) are rendered at 1.6 (far) / 1.0 (close) FPS (left), 2.0 / 1.2 FPS (center), and 1.0 / 0.9 FPS (right), respectively, at 4K (3840×2160) resolution. All examples use our balanced P-k-d tree, an acceleration structure which requires little or no memory cost beyond the original data.&lt;/i&gt;
					&lt;/figcaption&gt;
				&lt;/figure&gt;
				
				We present a novel approach to rendering large particle data sets from molecular dynamics, astrophysics and other sources. We employ a new data structure adapted from the original balanced k-d tree, which allows for representation of data with trivial or no overhead. In the OSPRay visualization framework, we have developed an efficient CPU algorithm for traversing, classifying and ray tracing these data. Our approach is able to render up to billions of particles on a typical workstation, purely on the CPU, without any approximations or level-of-detail techniques, and optionally with attribute-based color mapping, dynamic range query, and advanced lighting models such as ambient occlusion and path tracing.
				&lt;a href=&quot;https://www.willusher.io//publications/pkd&quot;&gt;Continue Reading&lt;/a&gt;
			</description>
			<link>https://www.willusher.io/publications/pkd</link>
			<guid>https://www.willusher.io/publications/pkd</guid>
			<pubDate>Sat, 24 Oct 2015 23:00:00 -0700</pubDate>
		</item>
		
	</channel>
</rss>
