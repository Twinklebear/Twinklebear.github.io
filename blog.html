

<!DOCTYPE html>
<html lang="en">
<head>
	<meta charset="utf-8">
	<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="origin-trial" content="AoMftca5+Dstn4K7mCd1AKVcenGb0/EBkqJgonux6w6fVXhyJ2iHs9LuZKA36+gcaypMdBLAaIpkES6VMXoJtg8AAABQeyJvcmlnaW4iOiJodHRwczovL3d3dy53aWxsdXNoZXIuaW86NDQzIiwiZmVhdHVyZSI6IldlYkdQVSIsImV4cGlyeSI6MTY0MzE1NTE5OX0=">

	<title>Blog</title>
	
    <meta name="author" content="Will Usher">

    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css" integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">

	<!-- Should I be including font-awesome from bootstrap? Or just the 5.0 version? -->
    <!--<script defer src="https://use.fontawesome.com/releases/v5.0.6/js/all.js"></script>-->
    <script defer src="https://kit.fontawesome.com/b56adde3a3.js" crossorigin="anonymous"></script>
    <script src="https://code.jquery.com/jquery-3.2.1.slim.min.js" integrity="sha384-KJ3o2DKtIkvYIK3UENzmM7KCkRr/rE9/Qpg6aAZGJwFDMVNA/GpGFF93hXpG5KkN" crossorigin="anonymous"></script>

	<link href="/assets/molokai.css" rel="stylesheet">
	<link href="/assets/custom.css" rel="stylesheet">
	<link rel="shortcut icon" href="/assets/img/identicon.ico">

</head>
<body>
	<nav class="navbar navbar-expand-md navbar-dark bg-dark static-top">
		<div class="container">
			<a class="navbar-brand" href="/">Will Usher</a>
			<button class="navbar-toggler navbar-toggler-right" type="button" data-toggle="collapse"
				data-target="#navbar_responsive" aria-controls="navbarResponsive" aria-expanded="false"
				aria-label="Toggle navigation">
					<span class="navbar-toggler-icon"></span>
			</button>
			<div class="collapse navbar-collapse" id="navbar_responsive">
				<ul class="navbar-nav mr-auto">
				
				
				  
					
					  
					  
						<li class="nav-item"><a href="/projects" class="nav-link">
							Projects</a></li>
						
					  
					
				  
					
					  
					  
						<li class="nav-item active"><a href="/blog" class="nav-link">
							Blog</a></li>
						
					  
					
				  
					
					  
					  
						<li class="nav-item"><a href="/pages/sdl2" class="nav-link">
							SDL2 Tutorials</a></li>
						
					  
					
				  
					
					  
					  
						<li class="nav-item"><a href="/news" class="nav-link">
							News</a></li>
						
					  
					
				  
					
					  
					  
						<li class="nav-item"><a href="/archive" class="nav-link">
							Archive</a></li>
						
					  
					
				  
				</ul>
			</div><!-- nav.collaps -->
		</div>
	</nav>

	<div class="container content mb-4">
		


<div class="row mt-4 justify-content-center">
	
	<div class="col-9 mb-2">
		<a href="/graphics/2023/05/16/0-to-gltf-first-mesh"><h3>From 0 to glTF with WebGPU: Rendering the First glTF Mesh</h3></a>
        <p>May 16, 2023</p>
		
<p>Now that we’ve seen how to draw a triangle in the <a href="https://www.willusher.io/graphics/2023/04/10/0-to-gltf-triangle">first post</a> and hook up camera controls so we can look around in the <a href="https://www.willusher.io/graphics/2023/04/11/0-to-gltf-bind-groups">second post</a>, we’re at the point where the avocado really hits the screen and we can start drawing our first glTF primitives! I say the avocado hits the screen because that’s the glTF test model we’ll be using. You can grab it from the <a href="https://github.com/KhronosGroup/glTF-Sample-Models/tree/master/2.0/Avocado">Khronos glTF samples repo</a>. glTF files come in two flavors (minus other extension specific versions), a standard “.gltf” version that stores the JSON header in one file and binary data and textures in separate files, and a “.glb” version, that combines the JSON header and all binary or texture data into a single file. We’ll be loading .glb files in this series to simplify how many files we have to deal with to get a model into the renderer, so grab the glTF-Binary <a href="https://github.com/KhronosGroup/glTF-Sample-Models/raw/master/2.0/Avocado/glTF-Binary/Avocado.glb">Avocado.glb</a> and let’s get started!</p>

<figure>
	<img class="img-fluid" src="https://cdn.willusher.io/webgpu-0-to-gltf/first-mesh-avocado.png" />
	
	<figcaption><b>Figure 1:</b>
	<i>It takes quite a bit to get Avocado.glb on the screen, but this
    beautiful image of our expected final (and delicious) result should
    be enough motivation to keep us going!
    </i></figcaption>
</figure>



		<a href="/graphics/2023/05/16/0-to-gltf-first-mesh">Continue &nbsp;<i class="fa fa-chevron-right" aria-hidden="true"></i></a>
		<div class="col-12 mt-2 mx-auto border-bottom border-secondary">
		</div>
	</div>
	
	<div class="col-9 mb-2">
		<a href="/graphics/2023/04/11/0-to-gltf-bind-groups"><h3>From 0 to glTF with WebGPU: Bind Groups - Updated for Chrome 113 Release</h3></a>
        <p>April 11, 2023</p>
		
<p>This tutorial is an updated version of my previous one and updates the
code listing to match the finalizing WebGPU APIs. If you’ve read
the previous version of this tutorial you can skim through the code
listings to get up to date.
The code for the blog series is also available <a href="https://github.com/Twinklebear/webgpu-0-to-gltf">on GitHub</a>.</p>

<p>In this second post of the series we’ll learn about Bind Groups,
which let us pass buffers and textures to our shaders.
When writing a renderer, we typically have inputs which do not make sense as vertex
attributes (e.g., transform matrices, material parameters), or simply cannot be passed
as vertex attributes (e.g., textures). Such parameters are instead
passed as uniforms in GLSL terms, or root parameters in HLSL terms.
The application then associates the desired buffers and textures with the
parameters in the shader. In WebGPU, the association of data to parameters is made using Bind Groups.
In this post, we’ll use Bind Groups to pass a uniform buffer containing a view
transform to our vertex shader, allowing us to add camera controls to our triangle
from the previous post.
If you haven’t read the <a href="/graphics/2023/04/10/0-to-gltf-triangle">updated first post in this series</a>
I recommend reading that first, as we’ll continue directly off the code written there.</p>



		<a href="/graphics/2023/04/11/0-to-gltf-bind-groups">Continue &nbsp;<i class="fa fa-chevron-right" aria-hidden="true"></i></a>
		<div class="col-12 mt-2 mx-auto border-bottom border-secondary">
		</div>
	</div>
	
	<div class="col-9 mb-2">
		<a href="/graphics/2023/04/10/0-to-gltf-triangle"><h3>From 0 to glTF with WebGPU: The First Triangle - Updated for Chrome 113 Release</h3></a>
        <p>April 10, 2023</p>
		
<p>This tutorial is an updated version of my previous one and updates the
code listing to match the finalizing WebGPU APIs. If you’ve read
the previous version of this tutorial you can skim through the code
listings to get up to date.
For an easy way to get started, I recommend grabbing my
<a href="https://github.com/Twinklebear/webgpu-webpack-starter">WebGPU + webpack starter template</a>
which includes the code from this tutorial. You can start by deleting the code there and
rewriting it following this tutorial, or follow along in the code
as you read this page.
The code for the blog series is also available <a href="https://github.com/Twinklebear/webgpu-0-to-gltf">on GitHub</a>.</p>

<p>WebGPU is a modern graphics API for the web, in development by the
major browser vendors. When compared to WebGL, WebGPU provides more direct
control over the GPU to allow applications to leverage the hardware
more efficiently, similar to Vulkan and DirectX 12.
WebGPU also exposes additional GPU capabilities not available in WebGL, such as compute
shaders and storage buffers, enabling powerful GPU compute applications
to run on the web. As with the switch from OpenGL to Vulkan, WebGPU
exposes more complexity to the user than WebGL, though the API strikes
a good balance between complexity and usability, and overall is quite nice to work with.
In this series, we’ll learn the key aspects of WebGPU from the ground up,
with the goal of going from zero to a basic glTF model renderer.
This post marks our initial step on this journey, where we’ll setup
a WebGPU context and get a triangle on the screen.</p>



		<a href="/graphics/2023/04/10/0-to-gltf-triangle">Continue &nbsp;<i class="fa fa-chevron-right" aria-hidden="true"></i></a>
		<div class="col-12 mt-2 mx-auto border-bottom border-secondary">
		</div>
	</div>
	
	<div class="col-9 mb-2">
		<a href="/graphics/2020/12/20/rt-dive-m1"><h3>A Dive into Ray Tracing Performance on the Apple M1</h3></a>
        <p>December 20, 2020</p>
		
<p>The Apple M1 available in the MacBook Air, MacBook Pro 13”, and Mac Mini has
been the focus of a ton of benchmarking writeups and blog posts about
the new chip. The performance overall, and especially performance/watt,
that Apple has achieved with the chip is very impressive.
As a ray tracing person, what caught my eye the most was the
performance AnandTech reported in their
<a href="https://www.anandtech.com/show/16252/mac-mini-apple-m1-tested/2">CineBench benchmarks</a>.
These scores were 1.6x higher than I got on my old Haswell desktop and 2x
higher than my new Tiger Lake laptop!
I had also been interested in trying out the new
<a href="https://developer.apple.com/videos/play/wwdc2020/10012/">ray tracing API for Metal</a>
that was announced at WWDC this year,
which bears some resemblance to the DirectX, Vulkan, and OptiX GPU ray tracing APIs.
So, I decided to pick up a Mac Mini to do some testing
on my own interactive path tracing project,
<a href="https://github.com/Twinklebear/ChameleonRT">ChameleonRT</a>,
and to get it running on the new Metal ray tracing API.
In this post, we’ll take a look at the new Metal ray tracing
API to see how it lines up with DirectX, Vulkan, OptiX and Embree,
then we’ll make some fair (and some extremely unfair) ray tracing
performance comparisons against the M1.</p>



		<a href="/graphics/2020/12/20/rt-dive-m1">Continue &nbsp;<i class="fa fa-chevron-right" aria-hidden="true"></i></a>
		<div class="col-12 mt-2 mx-auto border-bottom border-secondary">
		</div>
	</div>
	
	<div class="col-9 mb-2">
		<a href="/general/2020/11/15/hw-accel-encoding-rpi4"><h3>Hardware Accelerated Video Encoding on the Raspberry Pi 4 on Ubuntu 20.04 64-bit</h3></a>
        <p>November 15, 2020</p>
		
<p>I recently picked up a Raspberry Pi 4 8GB model to use for some lightweight server tasks
on my home network. After setting up Pi-Hole, OpenVPN, Plex, and Samba,
I got curious about using it to re-encode some videos I had. The videos are on
an external drive being monitored by Plex and shared on the network by Samba,
and some are quite large since they’re at a (likely unnecessarily) high bitrate.
Trimming them down would help save a bit of space, and gives me an excuse to
play around with Python, FFmpeg, and the Pi’s hardware accelerated video encoder.
In this post, I’ll cover how to get FFmpeg setup to use the Pi 4’s video encoding
hardware on a 64-bit OS and the little encoding manager/dashboard, <a href="https://github.com/Twinklebear/fbed">FBED</a>,
that I put together to monitor the progress of the encoding tasks.</p>



		<a href="/general/2020/11/15/hw-accel-encoding-rpi4">Continue &nbsp;<i class="fa fa-chevron-right" aria-hidden="true"></i></a>
		<div class="col-12 mt-2 mx-auto border-bottom border-secondary">
		</div>
	</div>
	

	
	<div class="col-12 text-center mb-2">
		<h5>Find older posts in the <a href="/archive">Archive</a></h5>
	</div>
	
</div>


	</div>

    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.12.9/umd/popper.min.js" integrity="sha384-ApNbgh9B+Y1QKtv3Rn7W3mgPxhU9K/ScQsAP7hUibX39j7fakFPskvXusvfa0b4Q" crossorigin="anonymous"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/js/bootstrap.min.js" integrity="sha384-JZR6Spejh4U02d8jOt6vLEHfe/JQGiRRSQQxSfFWpi1MquVdAyjUar5+76PVCmYl" crossorigin="anonymous"></script>
    

    <script defer data-domain="willusher.io" src="https://pan.wushernet.com/js/script.js"></script>

</body>
</html>



